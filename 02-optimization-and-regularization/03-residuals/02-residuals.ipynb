{"cells":[{"cell_type":"markdown","metadata":{"id":"raNzocw4IIjF"},"source":["# Explanation\n","\n","Before residuals, deep feed-forward network suffered from the **vanishing and exploding gradient** problems.\n","\n","In deep networks, as gradients from the last layer get passed backward, they get slowly magnified my repeated large weights, or squashed by repeated small weights. Then, in early layers of the network gradients would have very large magnitudes (exploding), or be near zero (vanishing), neither of which are conducive to actual incremental updates to parameters in the right direction.\n","\n","This meant that networks couldn't get past a certain depth before they became impractical to optimize.\n","\n","Residuals are the solution to the vanishing and exploding gradient problems.\n","\n","### Intuition\n","\n","Theoretically, a deeper network should be able to model exactly what it's shallower counterpart could, since you the deeper network could just take the same values as the shallower network for all its layers, and then for the remaining layers, just set weights to the identity function.\n","\n","In practice, it's difficult for these networks to learn the identity function - it requires explicitly learning weights to map inputs to outputs through multiple transformations.\n","\n","The idea behind residuals is to make it easier for networks to learn this identity function if necessary, so that additional layers can learn to minimize the changes they make to inputs when they aren't as needed.\n","\n","Additionally, residuals allow the gradients to be passed farther back through the network without exploding or vanishing, allowing deeper layers to train with the same quality gradients as layers closer to the cost function.\n","\n","How do they accomplish this?\n","\n","They change the job of individual layers.\n","\n","In networks without residuals, a layer has to learn how to transform its inputs to create the desired outputs.\n","\n","With residuals, each layer instead needs to learn how far its inputs are from the desired output. It detects the \"difference.\" Then, instead of transforming the input entirely, it simply adds this difference to the input to turn it into the output.\n","\n","Conceptually, we can see how this makes it easier to learn the identity function - instead of the network having to learn the full transformation with all parameters to make the input transform into itself (using many parameters), a layer can instead just learn a \"difference\" of 0 between the input and output, leaving its inputs unmodified.\n","\n","In order to understand how this setup helps to pass gradients through to deeper layers, we need to turn to the math.\n","\n","### Math\n","\n","We can describe the behavior of standard layers as trying to learn to map its inputs $x$ to outputs by learning some desired function $H(x)$.\n","\n","Residuals instead allow layers to learn the difference between the output of this desired function and the current values of $x$, where this difference can be modeled by $F(x)$ in the following $F(x) = H(x) - x$.\n","\n","So, in order to fully model $H(x)$, our network computes $H(x) = F(x) + x$, in other words, it adds the inputs of each layer to its activations. This has the effect of letting our network model \"differences\" directly.\n","\n","We see another very important effect of this by looking at how gradients flow in this setup.\n","\n","Given that the output of the layer is a sum of its activations and inputs, we know that the gradient flowing through the layer will be distributed evenly to both the activations of the current layer, and the inputs to the current layer (ie. the activations of the previous layer), since summation of terms distributes gradients evenly.\n","\n","This means that the earlier layer receives the original gradient directly through this pathway, in addition to receiving gradients via its inputs to the next layer.\n","\n","If this structure is repeated through all layers of the network, it creates a \"residual pathway\" for gradients to flow backward through the entire network, allowing high quality gradients received by layers closer to the cost function to flow all the back to earlier layers too.\n","\n","Empirically, this enables much deeper networks to actually converge."]},{"cell_type":"markdown","metadata":{"id":"zR6cVFvwIIwj"},"source":["# My Notes\n","\n","## 📜 [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n","\n","> We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.\n","\n","Residuals are the answer to enabling much deeper neural networks with many layers (which empirically has shown to be critical to improvement) while still enabling successful optimization.\n","\n","> Driven by the significance of depth, a question arises: _Is learning better networks as easy as stacking more layers?_\n","\n","The vanishing/exploding gradient problem has made it hard to answer this - but **normalized initialization** and **intermediate normalization layers** (like BatchNorm) have solved this problem.\n","\n","However, while this has enabled deeper networks to converge, they suffer from _degradation_, where accuracy decreases after a certain point of increasing depth even on the training set.\n","\n","> There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model.\n","\n","Here, they provide a thought experiment to show that a deeper neural network is capable of modeling something at least as good as it’s shallower counter-part by simply turning any additional layers into identity mapping layers that just directly map their inputs to their outputs.\n","\n","This is partly where the intuition for residuals comes from.\n","\n","> Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping.\n","\n","Here is the core intuition behind residuals.\n","\n","Most feed-forward networks or layers attempt to learn some underlying desired function $H(x)$ by trying to learn the entire transformation $x → H(x)$ directly.\n","\n","![Screenshot 2024-05-09 at 11.38.41 AM.png](../../images/Screenshot_2024-05-09_at_11.38.41_AM.png)\n","\n","Instead, residuals allow the network to instead try to learn the necessary _residual_ instead, the different from the current output, or just the transformation $F(x)$ where $F(x) = H(x) - x$.\n","\n","Then, the output of the layer computes $H(x) = F(x) + x$, adding back $x$ to the computed residual.\n","\n","In the limit, this means that it’s very easy for the network to learn the identity function by making $F(x)$ effect converge to 0 where necessary.\n","\n","Critically, this also propagates gradients back throughout the residual pathway effectively as the gradient from the output of the layer gets propagated back to the previous layer via $x$ due to the addition in $H(x)$.\n","\n","These **shortcut connections** or **skip connections** provide no extra parameters to the network or training complexity, meaning SGD can still be used for the whole thing.\n","\n","> We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.\n","\n","### Deep Residual Learning\n","\n","> If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., $H(x) − x$.\n","\n","> So rather than expect stacked layers to approximate $H(x)$, we explicitly let these layers approximate a residual function $F(x) := H(x) − x$. The original function thus becomes $F(x)+x$.\n","\n","> Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different.\n","\n","> The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers. With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.\n","\n","Residuals make it easy for the network to drive certain layers down to the identity, meaning that even networks with too much depth for the actual problem should still be able to maintain high accuracy.\n","\n","> The dimensions of $x$ and $F$ must be equal. If this is not the case (e.g., when changing the input/output channels), we can perform a linear projection $W_s$ by the shortcut connections to match the dimensions.\n","\n","$y = F(x, {W_i}) + W_sx$\n","\n","### Results\n","\n","![Screenshot 2024-05-09 at 11.46.54 AM.png](../../images/Screenshot_2024-05-09_at_11.46.54_AM.png)\n","\n","In the standard network, the 34-layer performs more poorly than the 18-layer in training, demonstrating degradation.\n","\n","Meanwhile, the ResNet prevents degradation and the 34-layer actually performs better.\n","\n","There are three important findings here:\n","\n","> More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data. This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth.\n","\n","> Compared to it’s plain counterpart, the 34-layer ResNet reduces the top-1 error by 3.5%, resulting from the successfully reduced training error. This comparison verifies the effectiveness of residual learning on extremely deep systems.\n","\n","> Last, we also note that the 18-layer plain/residual net are comparably accurate, but the 18-layer ResNet converges faster (Fig. 4 right vs. left). When the net is “not overly deep” (18 layers here), the current SGD solver is still able to find good solutions to the plain net. In this case, the ResNet eases the optimization by providing faster convergence at the early stage.\n","\n","Even when residuals aren’t necessary for the network to converge properly, residuals still make convergence significantly faster.\n","\n","> But the small differences among A/B/C indicate that projection shortcuts are\n","> not essential for addressing the degradation problem.\n","\n","In terms of identity vs. projection shortcuts ($W_s$ is the identity matrix or different), projection shortcuts make insignificant difference in the residuals. The only time projection shortcuts are necessary is when dimensions are different.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}