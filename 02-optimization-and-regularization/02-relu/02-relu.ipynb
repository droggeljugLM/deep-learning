{"cells":[{"cell_type":"markdown","metadata":{"id":"aa6pBVVq-4qt"},"source":["# Explanation\n","\n","At the time of this paper, most neural networks had to be initialized with an unsupervised learning phase in order to effectively initialize weights and regularize during training, and networks that could do this (like deep belief networks) performed significantly better on supervised learning tasks.\n","\n","The ReLU activation function is designed to fix this gap in performance by allowing feed-forward networks to perform well despite not using this unsupervised learning step.\n","\n","### Intuition\n","\n","In the brain, only around 1-4% of a neurons are active at a time, rather than the entire brain firing for most concepts.\n","\n","This indicates that the brain makes a tradeoff on \"richness of representation\" - ie. using all neurons to represent concepts, in favor of energy efficiency.\n","\n","This tradeoff also enables \"sparsity\" in the brain - different concepts are represented by different combinations of neurons, which can allow concepts to be distinguished in the brain.\n","\n","However, in initial feed-forward networks, there was no easy mechanism for the network to form \"sparse\" representations like this, allowing it to easily distinguish between different concepts.\n","\n","Generally, most neurons were active in some capacity for all inputs, meaning representations were much more distributed throughout the network.\n","\n","The ReLU activation function changed this by enabling neurons to easily turn off completely, allowing smaller subsets of neurons to represent concepts - and this enabled sparsity in neural networks.\n","\n","### Math\n","\n","The default activation function used at the time of this paper was the $\\sigma$ sigmoid activation function. Notably $\\sigma(0) = 0.5$, so when these neurons have no meaningful data passed to their input, they still tend to stay somewhat active.\n","\n","This was the actual reason a large number of neurons in the network were active for each different input.\n","\n","The ReLU activation function changed this by explicitly introducing the ability for neurons to reach 0 for a large part of their domain.\n","\n","The ReLU function $\\textrm{rectifier}(x) = \\textrm{max}(0, x)$, so the function gates all negative inputs to 0.\n","\n","Importantly, when passing back gradients through the network via back-propagation, any neurons with zeroed out values complete clip the flow of gradients, enabling large groups of neurons to remain inactive for different concepts.\n","\n","This is what actually enabled sparsity, which improved the quality of representations formed in these networks.\n","\n","Additionally, because the ReLU eliminates a large percentage of neurons from receiving gradients during back-propagation, it also boost computational efficiency."]},{"cell_type":"markdown","metadata":{"id":"Kc93E5lj-DzA"},"source":["# My Notes\n","\n","## 📜 [Deep Sparse Rectified Neural Networks](https://www.researchgate.net/publication/215616967_Deep_Sparse_Rectifier_Neural_Networks)\n","\n","> Many differences exist between the neural network models used by machine learning researchers and those used by computational neuroscientists.\n","\n","This paper highlights the importance of areas where both neuroscience and machine learning researches are both interested, which point toward “computationally motivated principles of operation in the brain that can also enhance research in artificial intelligence”\n","\n","ReLU apparently bridges two of the gaps between neuroscience and practical ML.\n","\n","The backdrop of this paper is a world where an unsupervised pre-training step for all supervised learning tasks helped with weight initialization in regularization and these networks (like deep belief networks) performed significantly better on supervised learning tasks than networks without this step.\n","\n","ReLU is an attempt to close this gap in performance.\n","\n","### Background\n","\n","**1. Neuroscience Observations**\n","\n","> Studies on brain energy expense suggest that neurons encode information in a sparse and distributed way.\n","\n","Only around 1-4% of neurons are active at a time. This is a trade-off between “richness of representation” and low energy expenditure for action potentials.\n","\n","The sigmoid activation does not enable this, as it has a “steady-state regime around 1/2,” (small inputs get squashed to around 0.5) so all neurons fire at their saturation, which is biologically implausible and hurts optimization.\n","\n","Meanwhile, a common biological neuron activation function is the leaky integrate-and-fire which has a zero threshold.\n","\n","![Screenshot 2024-05-13 at 10.59.21 AM.png](../../images/Screenshot_2024-05-13_at_10.59.21_AM.png)\n","\n","**2. Sparsity**\n","\n","> We show here that using a rectifying non-linearity gives rise to real zeros of activations and thus truly sparse representations.\n","\n","This has the following benefits:\n","\n","(1) Information disentangling\n","\n","> One of the claimed objectives of deep learning algorithms is to disentangle the factors explaining the variations in the data.\n","\n","Without zeroing weights, any change in input modifies all parts of the representation. If the representation is instead sparse and robust to small input changes, the set of non-zero features is mostly conserved for small changes.\n","\n","(2) Efficient variable-size representation\n","\n","> Varying the number of active neurons allows a model to control the effective dimensionality of the representation for a given input and the required precision.\n","\n","Different inputs may contain a different about of information and should be represented by a variable size vector. Sparse coding allows the network to adapt to this need.\n","\n","(3) Linear separability\n","\n","> Sparse representations are also more likely to be linearly separable\n","\n","Which is good because linear separation is computationally simpler of a decision function to learn\n","\n","(4) Distributed but sparse\n","\n","> Dense distributed representations are the richest representations, being potentially exponentially more efficient than purely local ones. Sparse representations’ efficiency is still exponentially greater, with the power of the exponent being the number of non-zero features.\n","\n","Less activations to compute in feed-forward means less computation. But more importantly, more zeroed values means tons of values don’t need to be computed in back-propagation which is efficient.\n","\n","> Nevertheless, forcing too much sparsity may hurt predictive performance for an equal number of neurons, because it reduces the effective capacity of the model.\n","\n","### Deep Rectifier Networks\n","\n","**1. Advantages of Rectifier Neurons**\n","\n","The rectifier activation function $\\textrm{rectifier}(x) = \\textrm{max}(0,x)$ allows a network to easily obtain sparse representations.\n","\n","For example, on initialization ~50% of the hidden units output values will be zeroes.\n","\n","> Apart from being more biologically plausible, sparsity also leads to mathematical advantages.\n","\n","> For a given input, only a subset of neurons are active. Computation is. linear on this subset. Once the subset of neurons is selected, the output is a linear function of the input.\n","\n","> We can see the model as an _exponential number of linear models that share parameters_.\n","\n","> Because of this linearity, gradients flow well on the active paths of neurons (there is no gradient vanishing effect due to the activation non-linearities of sigmoid or tanh units).\n","\n","> Computations are also cheaper: there is no need for computing the exponential function in activations, and sparsity can be exploited.\n","\n","ie. the derivative of ReLU is computationally cheaper than tanh and sigmoid when you do have to calculate it, and most of the outputs are zeroed anyway so you don’t even have to calculate them.\n","\n","**2. Potential Problems**\n","\n","> One may hypothesize that the hard saturation at 0 may hurt optimization by blocking gradient back-propagation\n","\n","They tried soft-plus as a way to test this. It’s not an issue - hard zeros actually help supervised training.\n","\n","> We hypothesize that the hard non-linearities do not hurt _so long as the gradient can propagate along some paths_.\n","\n","> With the _credit and blame assigned to these ON units_ rather than distributed more evenly, we hypothesize that optimization is easier.\n","\n","> Another problem could arise due to the unbounded behavior of activations; one may thus want to use a regularizer to prevent potential numerical problems.\n","\n","So they use the $L_1$ penalty on the activation values which also promotes additional sparsity.\n","\n","### Conclusion\n","\n","> Sparsity and neurons operating mostly in a linear regime can be brought together in more biologically plausible deep neural networks\n","\n","> Rectifier units help to bridge the gap between unsupervised pre-training and no pre-training, which suggests that they may help in finding better minima during training.\n","\n","> Furthermore, rectifier activation functions have shown to be remarkably adapted to sentiment analysis, a text-based task with a very large degree of data sparsity.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}