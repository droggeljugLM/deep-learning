{"cells":[{"cell_type":"markdown","metadata":{"id":"uWizOrMq_M1u"},"source":["# Explanation\n","\n","The **word2vec** model introduces the concept of embeddings models, which form word representaiotns that preserve semantic and syntactic meaning.\n","\n","Specifically, the model can take in words and encode them into vectors that are meant to be _representations_ of those words. These representations have special properties, in that just like how certain words are related to each other, the representations of related words should also be related.\n","\n","A cool example that indicates the power of this construction is that the following vector computation holds true\n","\n","$$\\textrm{Vector}(\\textrm{\"King\"}) - \\textrm{Vector}(\\textrm{\"Man\"}) + \\textrm{Vector}(\\textrm{\"Woman\"}) = \\textrm{Vector}(\\textrm{\"Queen\"})$$\n","\n","This suggests that the vectors contain some information about the words that preserve semantic meaning of words in relation to each other.\n","\n","### Word2Vec\n","\n","This paper experiments with a variety of architectures to try to create these embedding models, and then evaluates the performance of the different approaches.\n","\n","The architectures vary in their implementations, but there are several commonalities between them that define how all embedding models work.\n","\n","First, the embedding models all compress the inputs into a $D$ dimensional subspace in a hidden layer, and then use further hidden layers or similarity calculations based on the vectors in the hdiden layer to accomplish language based tasks.\n","\n","This hidden layer is the representation layer where the embeddings are created - embeddings are actually just the vectors created by hidden layers in neural networks where the networks are force to learn useful representations to accomplish a task within a specific subspace. In these cases, the embeddings for each word would be $D$ dimensional.\n","\n","The most effective architecture in this paper, the continuous Skip-gram model uses the task of predicting what words will appear in the same contexts to form representations by trying to push embedding vectors of wrods appearing together closer together in the embedding space.\n","\n","This forms complex relationships between the embeddings of different words as each word gets modified by it's complex relationships with many other words, until the vector for each word contains rich information about it's meaning.\n","\n","In general, all the embedding models work by forcing words that appear frequently together in training to be forced into similar parts of the embedding space, implying that these words relate to each other\n","\n","### Phrase2Vec\n","\n","The **phrase2vec** model builds on word2vec, improving on some of it's implementations and also adding the ability to embed phrases in addition to just words. This is motivated by the fact that certain phrases (for example, the name of a city or a sports team) composed of individual words may actually adopt meanings completely unrelated to the words they're made up of, creating the need for more complex embeddings.\n","\n","Additionally, this implementation introduces a few optimizations on top of the original word2vec model. Most notably, it introduces the sub-sampling of frequent words (similar to removing stop-words in traditional NLP) so that embeddings of frequent words dont get pushed in random directions given their frequency."]},{"cell_type":"markdown","metadata":{"id":"-1sTShZasLxU"},"source":["# My Notes\n","\n","## 📜 [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)\n","\n","> We propose two novel model architectures for computing continuous vector representations of words from very large data sets\n","\n","> We show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\n","\n","> Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary.\n","\n","> This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data.\n","\n","> However, the simple techniques are at their limits in many tasks.\n","\n","In many domains, like transcribed speech data, there isn’t much high quality data, so more complex models may be necessary to represent the available.\n","\n","> Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities.\n","\n","> It was shown for example that _vector(”King”) - vector(”Man”) + vector(”Woman”)_ results in a vector that is closest to the vector representation of the word _Queen_\n","\n","> In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words.\n","\n","> We design a new comprehensive test set for measuring both syntactic and semantic regularities, and show that many such regularities can be learned with high accuracy.\n","\n","This paper tries to focus on creating word representations that preserve syntactic and semantic regularities between different words (the representations contain meeting, and arithmetic can be performed).\n","\n","> Moreover, we discuss how training time and accuracy depends\n","> on the dimensionality of the word vectors and on the amount of the training data.\n","\n","### Model Architectures\n","\n","> In this paper, we focus on distributed representations of words learned by neural networks.\n","\n","Neural networks have been shown to be far more effective and computationally efficient than previous approaches like LSA and LDA.\n","\n","> We will try to maximize the accuracy, while minimizing the computational complexity.\n","\n","Minimize the the number of parameters needed for the model to fully train while making sure it’s still accurate.\n","\n","**1. Feedforward Neural Net Language Model (NNLM)**\n","\n","> The probabilistic feedforward neural network language model has been proposed in. It consists of input, projection, hidden and output layers.\n","\n","> At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary.\n","\n","> The input layer is then projected to a projection layer P that has dimensionality N × D, using a shared projection matrix.\n","\n","> Moreover, the hidden layer is used to compute probability distribution over all the words in the\n","> vocabulary, resulting in an output layer with dimensionality V .\n","\n","The model consists of:\n","\n","(1) An input layer of N (often N=10) of the previous words in a one-hot-encoded 1-of-V encoding style for the vocabulary V\n","\n","(2) There’s a projection layer with dimensionality N × D that maps each vector in the input layer linearly onto a learned representation. This projection layer is the **embedding layer**.\n","\n","(3) Then, there’s a hidden layer which computes based on the projection layer and has some non-linearities.\n","\n","(4) Finally, there’s an output layer of dimension V, corresponding with the prediction of the next word.\n","\n","**2. Recurrent Neural Net Language Model (RNNLM)**\n","\n","> Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length, and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks.\n","\n","RNN used for this representation task since they’re theoretically better at word modeling.\n","\n","> The RNN model does not have a projection layer; only input, hidden and output layer. What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections.\n","\n","The embedding layer in this model is not a projection layer but is instead the layer passing information forward in time since it has to form short term memory of the past hidden state.\n","\n","**3. Parallel Training of Neural Networks**\n","\n","> To train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called DistBelief, including the feedforward NNLM and the new models proposed in this paper.\n","\n","### New Log-Linear Models\n","\n","> In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity.\n","\n","> The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model.\n","\n","Building models optimized for simplicity. While they may not be able to have as complex representations as neural networks for the task, their computational efficiency is a big benefit.\n","\n","**1. Continuous Bag-of-Words Model**\n","\n","> The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).\n","\n","> We call this architecture a bag-of-words model as the order of words in the history does not influence the projection.\n","\n","> Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word.\n","\n","In this model, we remove the non-linear layer for the sake of removing complexity, and each word uses the same projection matrix, rather than have it’s own matrix in the projection layer, meaning that there is no information about word position.\n","\n","**2. Continuous Skip-gram Model**\n","\n","> The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence.\n","\n","> More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word.\n","\n","> We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity.\n","\n","This model just tries to predict which words are nearby based on the current word and a certain range.\n","\n","### Results\n","\n","> ”What is the word that is similar to small in the same sense as biggest is similar to big?”\n","\n","> Somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words.\n","\n","> Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin.\n","\n","**1. Task Description**\n","\n","> To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions.\n","\n","![Screenshot 2024-05-15 at 1.49.07 PM.png](../../images/Screenshot_2024-05-15_at_1.49.07_PM.png)\n","\n","> Question is assumed to be correctly answered only if the closest word to the. vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes.\n","\n","> We believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric.\n","\n","**2. Maximization of Accuracy**\n","\n","> We have used a Google News corpus for training the word vectors. This corpus contains about 6B tokens. We have restricted the vocabulary size to 1 million most frequent words.\n","\n","> Increasing amount of training data twice results in about the same increase of computational complexity as increasing vector size twice.\n","\n","Increasing the vector size of word representations has the same effect as a larger training set.\n","\n","**3. Comparison of Model Architectures**\n","\n","The skip-gram model performs best overall.\n","\n","![Screenshot 2024-05-15 at 1.57.00 PM.png](../../images/Screenshot_2024-05-15_at_1.57.00_PM.png)\n","\n","### Examples of the Learned Relationships\n","\n","![Screenshot 2024-05-15 at 1.58.46 PM.png](../../images/Screenshot_2024-05-15_at_1.58.46_PM.png)\n","\n","> It is also possible to apply the vector operations to solve different tasks. For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector.\n","\n","### Conclusion\n","\n","> In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks.\n","\n","> We observed that it is possible to train high quality word vectors using very simple model architectures.\n","\n","> Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary.\n","\n","💬 **Comments**\n","\n","This paper introduces the intuition for embeddings - it’s actually not a model trained to produce embeddings, but a model trained for a task, and created in a way so that the model is forced to create embeddings somewhere in order to accomplish it’s goal.\n","\n","Each different model proposed in this paper takes a different approach to language modeling that forces the model to build it’s own embeddings, and the tradeoffs taken are for the sake of computational complexity.\n","\n","In general, as a model is forced to model certain relationships between words & tokens more accurately to accomplish some task, words that appear together or are contextually similar will be adjusted in the representation space to cluster more closely together, and hopefully, to model relevant syntactic and semantic relationships.\n","\n","It’s actually very surprising that this happens naturally in the way that the models learn.\n","\n","The intuition of embeddings is to isolate the available representation space models have to learn so that when they optimize their model of language in their representation space, that space can then be used practically.\n"]},{"cell_type":"markdown","source":["\n","## 📜 [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546)\n","\n","> The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.\n",">\n","\n","> In this paper we present several extensions that improve both the quality of the vectors and the training speed.\n",">\n","\n","This paper adds many optimizations to the skip-gram model introduced in the word2vec paper.\n","\n","> An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.\n",">\n","\n","> We present a simple method for finding phrases in text, and show\n","that learning good vector representations for millions of phrases is possible.\n",">\n","\n","Given the limitations on individual word representations, it also focused on how more complex phrases can be represented by embeddings.\n","\n","> Distributed representations of words in a vector space help learning algorithms to achieve better performance in natural language processing tasks by grouping similar words.\n",">\n","\n","This is the core intuition behind why embeddings are useful to models, and why they have the properties they do after training.\n","\n","> Unlike most of the previously used neural network architectures for learning word vectors, training of the Skip-gram model does not involve dense matrix multiplications.\n",">\n","\n","> This makes the training extremely efficient: an optimized single-machine implementation can train on more than 100 billion words in one day.\n",">\n","\n","> We show that subsampling of frequent words during training results in a significant speedup (around 2x - 10x), and improves accuracy of the representations of less frequent words.\n",">\n","\n","> In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) for training the Skip-gram model that results\n","in faster training and better vector representations for frequent words.\n",">\n","\n","One major focus of this paper is to improve the performance and training efficiency of the existing skip-gram model.\n","\n","> Using vectors to represent the whole phrases makes the Skip-gram model considerably more expressive.\n",">\n","\n","The other major focus is in using vectors to represent phrases rather than words, which enables the embeddings to represent much more.\n","\n","> The extension from word based to phrase based models is relatively simple. First we identify a large number of phrases using a data-driven approach, and then we treat the phrases as individual tokens during the training.\n",">\n","\n","> To evaluate the quality of the phrase vectors, we developed a test set of analogical reasoning tasks that contains both words and phrases.\n",">\n","\n","### The Skip-gram Model\n","\n","> The training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document.\n",">\n","\n","This task is especially conducive to creating useful context in the word embeddings - you need to very clearly be able to derive different types of related words from the embedding of each word.\n","\n","> Formally, given a sequence of training words $w_1, w_2, w_3, …, w_T$, the objective of the Skip-gram model is to maximize the average log probability\n",">\n","\n","$$\n","\\frac{1}{T}\\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j}|w_t)\n","$$\n","\n","Meaning, for each word (summation over $T$ terms), we want to increase the total chance that the model predicts the presence of all surrounding words within context window $c$ (summation over $j$ terms, bounded by $c$).\n","\n","This can be framed as a minimization by changing the main term to a $- \\log$ probability.\n","\n","> The basic Skip-gram formulation defines $p(w_{t+j}|w_t)$ using the softmax function:\n",">\n","\n","$$\n","p(w_O|w_I) = \\frac{\\exp({v'_{w_O}}^Tv_{w_I})}{\\sum_{w=1}^W \\exp{{v'_w}^Tv_{w_I}}}\n","$$\n","\n","> This formulation is impractical because the cost of computing $\\nabla \\log p(w_O|w_I)$ is proportional to $W$.\n",">\n","\n","The Skip-gram cost function is meant to maximize the models predicted probability of the presence of each of the words that’s in the actual $2c$ word context window.\n","\n","The model stores an embedding vector for each word in the vocabulary, and each output is computed by multiplying the dot product of the target word’s embedding vector with the embedding vector for each other word in the vocabulary.\n","\n","The model works as follows:\n","\n","(1) The input layer is a 1-of-V one hot encoded vector with V inputs ($V$ being the vocabulary size)\n","\n","(2) The projection layer directly maps each word in the input to an $D$ dimensional embedding (linear mapping). Thus this layer has dimension $V$by $D$. Since only one input neuron is active at a time, only one embedding row (the embedding of the target word) is active at once.\n","\n","(3) The output layer then computes the dot products of the embedding vectors of all other words with the target word, and then these scores are passed through the softmax function, indicating the probability of each word appearing in the context window of the target word\n","\n","Through this optimization, the cost function forces words that appear in similar contexts to be pushed into closer regions (increasing similarity) in the embedding space. This is the core intuition behind how embeddings spaces actually develop the emergent representations that they have.\n","\n","This probability is technically calculated using a softmax on the entire set of probabilities calculated by the model on the $W$ outputs (corresponding with each word in the vocabulary), which is a very computationally expensive calculation.\n","\n","Instead, approximation methods are used to calculate this probability more efficiently.\n","\n","**1. Hierarchical Softmax**\n","\n","> A computationally efficient approximation of the full softmax is the hierarchical softmax.\n",">\n","\n","> The main advantage is that instead of evaluating $W$ output nodes in the neural network to obtain the probability distribution, it is needed to evaluate only about $\\log_2(W)$ nodes.\n",">\n","\n","This method works by creating a binary tree representing the $W$ outputs where higher nodes summarize the joint probabilities of all of its child nodes. Then, only a subset of these nodes need to be traversed (low probability branches can be completely cut off), making the sampling far more efficient.\n","\n","**2. Negative Sampling**\n","\n","> An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE). […] NCE posits that a good model should be able to differentiate data from noise by means of logistic regression.\n",">\n","\n","NCE is a method to approximate the value of softmax. Instead of taking a softmax across a large number of logits to compute scores for each output, instead, each output becomes a sigmoid activated score computation, meant to indicate whether a word is a “context” word that is actually associated with the input word, or a “noise” word.\n","\n","All the words that are context words should converge toward being predicted as noise words. However, for efficiency, the noise words are randomly selected by a noise distribution.\n","\n","> While NCE can be shown to approximately maximize the log probability of the softmax, the Skip-gram model is only concerned with learning high-quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality.\n",">\n","\n","Instead of NCE, we use a simplified version called NEG\n","\n","$$\n","\\log \\sigma({v'_{w_O}}^Tv_{w_I}) + \\sum_{i=1}^k \\mathbb{E}_{w_i \\sim P_n(w)}[\\log \\sigma({-v'_{w_i}}^Tv_{w_I})]\n","$$\n","\n","Here, we want to maximize the probability $\\log \\sigma({v'_{w_O}}^Tv_{w_I})$, meaning we want to maximize the dot products (similarities) of the target word embedding vector with the embedding vectors of the correct context words.\n","\n","Additionally, for $k$ randomly sampled words from the noise distribution $P_n(w)$, we want to maximize the average $\\log \\sigma({-v'{w_i}}^Tv{w_I})$ of these $k$ words corresponding embedding vectors with the target words embedding vectors. This quantity tries to maximize the similarity between the *opposite* of the noise words embedding vectors and target words embedding vector, or effectively minimizes the similarity of the two embedding vectors.\n","\n","> The main difference between the Negative sampling and NCE is that NCE needs both samples and the numerical probabilities of the noise distribution, while Negative sampling uses only samples. And while NCE approximately maximizes the log probability of the softmax, this property is not important for our application.\n",">\n","\n","In order for NCE to effectively mirror the probability distribution that would be learned if the softmax function were used, we would have to use correct numerical probabilities from the noise distribution, but since we don’t need this level of accuracy for this case (since the goal is just to create good embedding vector representations), the NEG function is sufficient.\n","\n","> Both NCE and NEG have the noise distribution $P_n(w)$ as a free parameter. We investigated a number of chocies for $P_n(w)$ and found that the unigram distribution […] $U(w)^{3/4}/Z$ outperformed significantly […] on every task we tried.\n",">\n","\n","**3. Subsampling of Frequent Words**\n","\n","> In very large corpora, the most frequent words can easily occur hundreds of millions of times. […] Such words usually provide less information value than the rare words.\n",">\n","\n","> While the skip-gram model benefits from observing the co-occurrences of “France” and “Paris”, it benefits much less from observing the frequent co-occurrences of “France” and “the”, as nearly every word co-occurs frequently within a sentence with “the.”\n",">\n","\n","> The vector representations of frequent words do not change significantly after training on several million examples.\n",">\n","\n","Frequent words in the corpus don’t add much information to the embeddings of other words, and they don’t soak up context from all the variety of words that surround them.\n","\n","> To counter the imbalance between the rare and frequent words, we used a simple subsampling approach: each word in the training set is discarded with probability computed by the formula\n",">\n","\n","$$\n","P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}\n","$$\n","\n","> where $f(w_i)$ is the frequency of word $w_i$ and $t$ is a chosen threshold, typically around $10^{-5}$\n",">\n","\n","The most frequent words beyond the threshold are highly likely to be sampled out of the data set, and the order of word frequencies is still preserved.\n","\n","This effectively compresses the frequencies in the dataset to maintain the same order but have much smaller variance.\n","\n","> It accelerates learning and even significantly improves the accuracy of the learned vectors of the rare words, as will be shown in the following sections.\n",">\n","\n","### Empirical Results\n","\n","![Screenshot 2024-05-15 at 4.24.44 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/d53d4b03-ec38-429d-9915-08fc298fc6e9/a45e6fc0-ef0e-4d06-b56d-657b4f752c5a/Screenshot_2024-05-15_at_4.24.44_PM.png)\n","\n","> The table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical reasoning task, and has even slightly better performance than the Noise Contrastive Estimation.\n",">\n","\n","> The subsampling of the frequent words improves the training speed several times and makes the word representations significantly more accurate.\n",">\n","\n","### Learning Phrases\n","\n","> As discussed earlier, many phrases have a meaning that is not a simple composition of the meanings of its individual words. To learn vector representation for phrases, we first find words that appear frequently together, and infrequently in other contexts.\n",">\n","\n","> Phrases are formed based on the unigram and bigram counts. The $\\delta$ is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed.\n",">\n","\n","$$\n","\\textrm{score}(w_i, w_j) = \\frac{\\textrm{count}(w_iw_j) - \\delta}{\\textrm{count}(w_i) \\times \\textrm{count}(w_j)}\n","$$\n","\n","> The bigrams with score above the chosen threshold are then used as\n","phrases.\n",">\n","\n","Each phrase is then replaced with it’s own (new) token in the dataset.\n","\n","**1. Phrase Skip-Gram Results**\n","\n","> Surprisingly, while we found the Hierarchical Softmax to achieve lower performance when trained without subsampling, it became the best\n","performing method when we downsampled the frequent words.\n",">\n","\n","![Screenshot 2024-05-15 at 4.41.55 PM.png](../../images/Screenshot_2024-05-15_at_4.41.55_PM.png)\n","\n","![Screenshot 2024-05-15 at 4.39.09 PM.png](../../images/21c8f6ba-e040-483e-9fef-836a63aed4fd/Screenshot_2024-05-15_at_4.39.09_PM.png)\n","\n","### Additive Compositionality\n","\n","> We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit a linear structure that makes it possible to perform precise analogical reasoning using simple vector arithmetics.\n",">\n","\n","> Interestingly, we found that the Skip-gram representations exhibit another kind of linear structure that makes it possible to meaningfully combine words by an element-wise addition of their vector representations.\n",">\n","\n","The created model also allows words to be added together.\n","\n","> The additive property of the vectors can be explained by inspecting the training objective.\n",">\n","\n","The training objectives in the creation of embedding models are heavily responsible for the resulting behaviors that are viable in the representation space.\n","\n","### Comparison to Published Word Representations\n","\n","![Screenshot 2024-05-15 at 4.48.17 PM.png](../../images/Screenshot_2024-05-15_at_4.48.17_PM.png)\n","\n","### Conclusion\n","\n","> This work has several key contributions. We show how to train distributed representations of words and phrases with the Skip-gram model and demonstrate that these representations exhibit linear structure that makes precise analogical reasoning possible.\n",">\n","\n","> A very interesting result of this work is that the word vectors can be somewhat meaningfully combined using just simple vector addition.\n",">\n","\n","> Another approach for learning representations of phrases presented in this paper is to simply represent the phrases with a single token. Combination of these two approaches gives a powerful yet simple way how to represent longer pieces of text, while having minimal computational complexity\n",">\n","\n","💬 **Comments**\n","\n","The quality and properties of the embedding model is a result of the specific training methods and objective functions used for the model. The relationships between words are enforced by the types of representations learned to model the training problem.\n","\n","This paper mainly introduces the technical details of the skip-gram model and how it’s design is conducive to learning good word embeddings.\n","\n","It also shows us how to add the ability to embed complex phrases into the embeddings model."],"metadata":{"id":"7wB2aC9gYt0h"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}