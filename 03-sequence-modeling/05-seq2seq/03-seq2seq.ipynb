{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBjlnxDXuvo8WFLi3H21Zr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Explanation\n","\n","The **encoder-decoder** model presented a powerful architecture for machine translation based on a two-part model using RNNs. This architecture was then developed on in the **seq2seq** model, which introduced an LSTM based version of the encoder-decoder architecture, further improving on the machine translation task.\n","\n","While impactful on their own, these papers set the foundation for the critical _Attention Is All You Need_ paper which introduced the transformer.\n","\n","### Encoder-Decoder\n","\n","The encoder-decoder architecture introduced the new structure of two RNNs working together to accomplish the machine translation task. Specifically, one RNN, known as the encoder, takes in an input sequence and produces a vector representing the necessary information to produce the next token in the sequence.\n","\n","The decoder then takes in this vector and produces the actual next token.\n","\n","This combination proved effective at predicting next words in machine translation. Additionally, it could be used to evaluate the quality of translations, allowing it to become part of a broader statistical machine translation (SMT) setup with other non-deep-learning parts (this is the type of setup that google translate uses, for example).\n","\n","### Seq2Seq\n","\n","The seq2seq model directly builds on encoder-decoder architecture by keeping the structure the same, but replacing the encoder and decoder models for LSTMs instead of RNNs.\n","\n","The encoder LSTM still maps the input sequences into a lower dimensional representation vector meant to contain all the context necessary for prediction of the next words in translation.\n","\n","The decoder LSTM then used this memory representation vector to predict the next word.\n","\n","The main difference in design the seq2seq model introduced was that it feeds words in the input sequence to the network in reverse order.\n","\n","Emprically, this seems to work because it puts initial words in the output sequence (the first words to translate) temporally close to the words in the input sequence.\n","\n","Another important results of the encoder-decoder architecture noted here is that fitting the encodings into a representation space creates a constraint conducive for meaningful embeddings as the output of the encoder."],"metadata":{"id":"575bYpbmYZr1"}},{"cell_type":"markdown","source":["# My Notes\n","\n","## 📜 [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078) [Encoder-Decoder]\n","\n","> In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks.\n",">\n","\n","> One RNN encodes a sequence of symbols into a fixed length vector representation, and the other decodes the representation into another sequence of symbols.\n",">\n","\n","> The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence.\n",">\n","\n","> Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.\n",">\n","\n","Similar to the embeddings papers - it appears that this model creates a constraint to create a good sentence embedding model at the point where representations are created by the encoder, where these embeddings are forced to contain information about the meaning of a sentence, and then these embeddings are re-expressed in the target language with the decoder.\n","\n","> Along this line of research on using neural networks for SMT, this paper focuses on a novel neural network architecture that can be used as a part of the conventional phrase-based SMT system.\n",">\n","\n","> Additionally, we propose to use a rather sophisticated hidden\n","unit in order to improve both the memory capacity\n","and the ease of training.\n",">\n","\n","### RNN Encoder-Decoder\n","\n","**2. RNN Encoder-Decoder**\n","\n","> The encoder is an RNN that reads each symbol of an input sequence x sequential. […] After reading the end of the sequence, the hidden state of the RNN is a summary $c$ of the whole input sequence.\n",">\n","\n","> The decoder of the proposed model is another RNN which is trained to generate the output sequence by predicting the next symbol $y_t$ given the hidden state $h_{(t)}$. […] Both $y_t$ and $h_{(t)}$ are also conditioned on $y_{t-1}$ and on the summary $c$ of the input sequence.\n",">\n","\n","> The two components of the proposed RNN Encoder–Decoder are jointly trained to maximize the conditional log-likelihood.\n",">\n","\n","$$\n","\\max_\\theta \\frac{1}{N}\\sum_{n=1}^N \\log p_\\theta(y_n|x_n)\n","$$\n","\n","> Once the RNN Encoder–Decoder is trained, the model can be used in two ways. One way is to use the model to generate a target sequence given an input sequence. On the other hand, the model can be used to score a given pair of input and output sequences, where the score is simply a probability $p_\\theta(y|x)$.\n",">\n","\n","The model can be used to do actual generations, and also to score the likelihood that one sequence is the translation of another (used later in the Seq2Seq paper).\n","\n","**3. Hidden Unit that Adaptively Remembers and Forgets**\n","\n","> In addition to a novel model architecture, we also propose a new type of hidden unit that has been motivated by the LSTM unit but is much simpler to compute and implement.\n",">\n","\n","They add a reset and update gate to a new unit in the RNN.\n","\n","### Statistical Machine Translation\n","\n","> In a commonly used statistical machine translation system (SMT), the goal of the system (decoder, specifically) is to find a translation $f$ given a source sentence $e$, which maximizes\n",">\n","\n","$$\n","p(f|e) \\propto p(e|f)p(f)\n","$$\n","\n","The $p(e|f)$ term is known as the *translation model* and determines whether a given translation is likely to be equivalent to the source sentence.\n","\n","The $p(f)$ term is the *language model* and determines how grammatically and syntactically correct a sentence is in the target language.\n","\n","> Most SMT systems model $\\log p(f | e)$ as a log-linear model with addition features and corresponding weights\n",">\n","\n","$$\n","\\log p(f|e) = \\sum_{n=1}^N w_nf_n(f,e) + \\log Z(e)\n","$$\n","\n","> Recently, […] there has been interest in training neural networks to score the translated sentence (or phrase pairs) using a representation of the source sentence as an additional input.\n",">\n","\n","**1. Scoring Phrase Pairs with RNN Encoder-Decoder**\n","\n","> Here we propose to train the RNN Encoder–Decoder on a table of phrase pairs and use its scores as additional features in the log-linear model when tuning the SMT decoder.\n",">\n","\n","In this section, they propose using the RNN Encoder-Decoder’s ability to produce scores for different translations as another factor to contribute to the broader structure of a more traditional SMT model.\n","\n","> With a fixed capacity of the RNN Encoder–Decoder, we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities.\n",">\n","\n","### Experiments\n","\n","**4. Word and Phrase Representations**\n","\n","> It has been known for some time that continuous space language models using neural networks are able to learn semantically meaningful embeddings.\n",">\n","\n","> From the visualization, it is clear that the RNN Encoder–Decoder captures both semantic and syntactic structures of the phrases.\n",">\n","\n","![Screenshot 2024-05-15 at 6.02.30 PM.png](../../images/Screenshot_2024-05-15_at_6.02.30_PM.png)\n","\n","### Conclusion\n","\n","> The proposed RNN Encoder–Decoder is able to either score a pair of sequences (in terms of a conditional probability) or generate a target sequence given a source sequence.\n",">\n","\n","> Our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as well as phrase level.\n"],"metadata":{"id":"rnbDWoaXsgI0"}},{"cell_type":"markdown","source":["## 📜 [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215) [Seq2Seq]\n","\n","> In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.\n","\n","> Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.\n","\n","This uses the encoder-decoder architecture with LSTMs for sequence-to-sequence tasks.\n","\n","> Given that translations tend to be paraphrases of the source sentences, the translation objective encourages the LSTM to find sentence representations that capture their meaning, as sentences with similar meanings are close to each other while different sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model is aware of word order and is fairly invariant to the active and passive voice.\n","\n","### The Model\n","\n","The three main changes from the default LSTM formulation.\n","\n","> First, we used two different LSTMs: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously.\n","\n","> Second, we found that deep LSTMs significantly outperformed shallow LSTMs, so we chose an LSTM with four layers.\n","\n","> Third, we found it extremely valuable to reverse the order of the words of the input sentence.\n","\n","### Experiments\n","\n","**1. Dataset Details**\n","\n","> We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words.\n","\n","**2. Decoding and Rescoring**\n","\n","> Once training is complete, we produce translation by finding the most likely translation according to the LSTM\n","\n","$$\n","\\hat{T} = \\arg \\max_T p(T|S)\n","$$\n","\n","> We search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number $B$ of partial hypotheses, where a partial hypothesis is a prefix of some translation.\n","\n","> As soon as the “<EOS>” symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses.\n","\n","Beam search is used to traverse the tree of sequence predictions to create a number of hypotheses, and then the available hypotheses that get created can get scored and selected from.\n","\n","**3. Reversing the Source Sentences**\n","\n","> We discovered that the LSTM learns much better when the source sentences are reversed.\n","\n","> While we do not have a complete explanation to this phenomenon, we believe that it is caused by the introduction of many short term dependencies to the dataset.\n","\n","As with all attempts at interpretability in deep learning research, there are results that are empirically definitive, but the explanations are not clear. Attempts at rationalization are made post-hoc.\n","\n","> Normally, when we concatenate a source sentence with a target sentence, each word in the source sentence is far from its corresponding. word in the target sentence. As a result, the problem has a large “minimal time lag.”\n","\n","By reversing the sentence order, this “minimal time lag” is reduced, which could impact backpropagation.\n","\n","> Backpropagation has an easier time “establishing communication” between the source sentence and the target sentence, which in turn results in substantially improved overall performance.\n","\n","**6. Experimental Results**\n","\n","> While the decoded translations of the LSTM ensemble do not outperform the best WMT’14 system, it is the first time that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT task by a sizeable margin, despite its inability to handle out-of-vocabulary words.\n","\n","![Screenshot 2024-05-15 at 5.25.40 PM.png](../../images/Screenshot_2024-05-15_at_5.25.40_PM.png)\n","\n","**8. Model Analysis**\n","\n","> One of the attractive features of our model is its ability to turn a sequence of words into a vector of fixed dimensionality.\n","\n","![Screenshot 2024-05-15 at 5.21.38 PM.png](../../images/Screenshot_2024-05-15_at_5.21.38_PM.png)\n","\n","![Screenshot 2024-05-15 at 5.22.18 PM.png](../../images/Screenshot_2024-05-15_at_5.22.18_PM.png)\n","\n","### Conclusion\n","\n","> In this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes almost no assumption about problem structure can outperform a standard SMT-based system whose vocabulary is unlimited on a large-scale MT task.\n","\n","> We were surprised by the extent of the improvement obtained by reversing the words in the source sentences. We conclude that it is important to find a problem encoding that has the greatest number of short term dependencies, as they make the learning problem much simpler.\n","\n","> We were also surprised by the ability of the LSTM to correctly translate very long sentences.\n","\n","> Most importantly, we demonstrated that a simple, straightforward and a relatively unoptimized approach can outperform an SMT system, so further work will likely lead to even greater translation accuracies."],"metadata":{"id":"vzoteErQcJtH"}}]}