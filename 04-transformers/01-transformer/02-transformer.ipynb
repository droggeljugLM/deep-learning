{"cells":[{"cell_type":"markdown","metadata":{"id":"wyQmy2xym86x"},"source":["# Explanation\n","\n","Building on the previous progress made from the Seq2Seq & Attention papers in the previous section, the Transformer improves on the state of the art for the machine translation task.\n","\n","The title of the paper _Attention Is All You Need_ reflects on the main idea governing the transformer architecture - the \"attention\" mechanism introduced in a previous paper along with recurrence was actually sufficient to perform better alone on the machine translation task.\n","\n","The transformer architecture builds around this by removing recurrence entirely and just using attention alone.\n","\n","### Resources\n","\n","Given the popularity of transformers, there are lot's of great resources for understanding the intuitions along with the software layer of how transformers work.\n","\n","I'd especially recommend watching these videos:\n","- [But what is a GPT? Visual intro to transformers](https://www.youtube.com/watch?v=wjZofJX0v4M)\n","- [Attention in transformers, visually explained](https://www.youtube.com/watch?v=eMlx5fFNoYc)\n","- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n","- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)\n","\n","### Intuition\n","\n","The previous initial architecture that introduced attention used the encoder to create a set of _annotations_ for each word, and then computed a context vector summarizing the relevant information from these annotations important to predicting the next word by taking a weighted sum of these annotations.\n","\n","In other words, each time a new word needed to be generated, the decoder would look through all the previous words to determine which words were relevant to the meaning of the next word.\n","\n","The transformer takes this a step farther by enabling not just the next word, but all words in the sequence to attend to each other and enhance each others meaning.\n","\n","All words (technically tokens) in the sequence have a chance to search for any other words the sequence relevant to them, and then they adopt parts of these words meanings into their own meanings.\n","\n","To understand how this works specifically, we can look at the math\n","\n","### Math\n","\n","A single attention head is computed as follows:\n","\n","$$\n","\\textrm{Attention}(Q,K,V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n","$$\n","\n","Here, we have a matrix of queries (Q), keys (K), and values (V) that each contain information for each token in the sequence.\n","\n","Specifically, the queries contain some information about what types of tokens are relevant to each token in the sequence, the keys contain information about what types of tokens each token in the sequence is relevant to, and the values contain some information about the meaning of each token.\n","\n","By taking the $QK^T$, we compute the relevance of the meaning of each word to each other word. Then, when we multiply the result of this multiplication with the values via $\\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$, each token contributes meanings to each other token in proportion to it's relevance.\n","\n","Thus, after an attention head is applied to the tokens, each tokens meaning is enhanced with the context of all the other important information around it.\n","\n","For each token, we take the softmax of all the $QK^T$ values for the token corresponding to the queries to weight the relative importance of different words to this token, forcing the most important tokens to be emphasized.\n","\n","We divide the $QK^T$ by $\\sqrt{d_k}$ to ensure that these quantities don't saturate in the softmax (ie. they don't hit the part of the softmax function that's so flat that gradients start to vanish)."]},{"cell_type":"markdown","metadata":{"id":"zlaoIsjxmzXN"},"source":["# My Notes\n","\n","## ðŸ“œ [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)\n","\n","> We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n","\n","Building on the state of encoder-decoder architectures, the Transformer removes all the complexities and uses attention as the only source of computation.\n","\n","> On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n","\n","This is an insane result showing the huge efficiency jump due to parallelization that the transformer offers - better performance in a fraction of the time.\n","\n","> Recurrent models typically factor computation along the symbol positions of the input and output sequences. [â€¦] The fundamental constraint of sequential computation, however, remains.\n","\n","The fundamental computational constraints of recurrent models makes them inefficient, despite optimization attempts.\n","\n","> In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n","\n","Motivated by the inefficiency of recurrence, this paper completely gets rid of recurrence and uses only attention to form representations.\n","\n","> The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n","\n","> In the Transformer [the number of operations required to relate signals from two arbitrary input or output positions] is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention\n","\n","In the Transformer, attention in a single head takes an average of the relevant information at all positions to compute the context to update individual word embedding vectors with, which can have the effect of reduce resolution. To counter-act this, multi-headed attention creates another way for the transformer to focus on multiple different important types of information for each word.\n","\n","> Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n","\n","Individual words in a sequence can soak up context from each other to enrich their own meanings.\n","\n","### Model Architecture\n","\n","> The Transformer follows this overall architecture [of an encoder-decoder model] using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n","\n","![Screenshot 2024-05-15 at 11.49.10â€¯PM.png](../../images/Screenshot_2024-05-15_at_11.49.10_PM.png)\n","\n","**1. Encoder and Decoder Stacks**\n","\n","> The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\n","\n","Each of the $N$ layers contains a multi-headed attention block for the words in the input sequence to self-attend to each other and soak up meanings from each other, as well as a feed forward network to use and interpret those meanings.\n","\n","Additionally, each sub-layer uses layer normalization and residuals for optimization purposes.\n","\n","> The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n","\n","Masked multi-head attention is used in the decoder to ensure that output words canâ€™t attend to words that follow them\n","\n","**2. Attention**\n","\n","> An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n","\n","**2.1. Scaled Dot-Product Attention**\n","\n","![Screenshot 2024-05-15 at 11.54.43â€¯PM.png](../../images/Screenshot_2024-05-15_at_11.54.43_PM.png)\n","\n","$$\n","\\textrm{Attention}(Q,K,V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n","$$\n","\n","Both dot-product and additive attention were viable choices. However:\n","\n","> Dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n","\n","> We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n","> extremely small gradients. To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$.\n","\n","The scaling factor is just to keep the outputs of the softmax function in the regime where itâ€™s gradient doesnâ€™t vanish, regardless of the dimensions size (by minimizing the differences between values).\n","\n","**2.2. Multi-Head Attention**\n","\n","> Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n","\n","The primary affect of multi-headed attention is to enable the model to soak up context from multiple distinct potential information sources for each word - something that couldnâ€™t happen as easily with just a single head due to the fact that individual heads take weighted averages of the\n","\n","$$\n","\\textrm{MultiHead}(Q,K,V) = \\textrm{Concat}(\\textrm{head}_1,...,\\textrm{head}_h)W^O \\\\\n","\\textrm{where head}_i = \\textrm{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n","$$\n","\n","Each multi-headed attention block concatenates the results of each of its head into the final vector thatâ€™s passed to the feed-forward layer.\n","\n","This means that the feed-forward layer is actually not just receiving a single opinion about the words enriched context, but actually a completely different perspective about the words meaning for each head.\n","\n","In this case, that means that the feed-forward layer is actually receiving 8 different perspectives on the enriched meaning of each token.\n","\n","> Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n","\n","**2.3. Applications of Attention in our Model**\n","\n","> In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n","\n","$$\n","\\textrm{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n","$$\n","\n","> While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n","\n","Because the same weight matrix is applied to each enhanced token outputted from the multi-headed attention layer, this can be thought of as a convolution being applied to each position.\n","\n","**4. Embeddings and Softmax**\n","\n","> We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n","\n","**5. Positional Encodings**\n","\n","> Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.\n","\n","This is what enables Transformers to still maintain information about word positions without explicit recurrence.\n","\n","> In this work, we use sine and cosine functions of different frequencies:\n","\n","$$\n","PE_{(pos,  2i)} = \\sin(pos/1000^{2i/d_{\\textrm{model}}}) \\\\\n","PE_{(pos,  2i+1)} = \\cos(pos/1000^{2i/d_{\\textrm{model}}})\n","$$\n","\n","### Why Self-Attention\n","\n","> Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network.\n","\n","> Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\n","\n","Transformers make it far easier for the network to learn long-range dependencies between words since thereâ€™s no recurrence to make path lengths across time long.\n","\n","> To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective output position.\n","\n","This is the motivation behind limited context windows.\n","\n","> As side benefit, self-attention could yield more interpretable models. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n","\n","Multi-headed attention appears to model highly interpretable behaviors, something uncharacteristic of most models.\n","\n","### Training\n","\n","> We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\n","\n","> Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens.\n","\n","> We trained our models on one machine with 8 NVIDIA P100 GPUs. [â€¦] Each training step took about 0.4 seconds.\n","\n","> We used the Adam optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.98$ and $\\epsilon = 10^{âˆ’9}$.\n","\n","> We apply dropout to the output of each sub-layer before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n","\n","> During training, we employed label smoothing of value $\\epsilon_{l_s} = 0.1$.\n","\n","### Results\n","\n","> Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n","\n","> To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n","\n","> Despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar.\n","\n","### Conclusion\n","\n","> In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n","\n","> For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n"]},{"cell_type":"markdown","metadata":{"id":"Y67MJmYjmYnP"},"source":["# Implementation\n","\n","This implementation is based on Andrej Karpathy's transformer tutorial: [let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY). All of his tutorials are great for building intuitions and he explains all the concepts in depth."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1716096031045,"user":{"displayName":"Adam Majmudar","userId":"11411695890251280564"},"user_tz":420},"id":"rKknTDT8MW77","outputId":"2e544379-5fa7-47ed-9ebd-5b80526bd24f"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-05-19 05:20:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: â€˜input.txtâ€™\n","\n","input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n","\n","2024-05-19 05:20:30 (163 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716096033600,"user":{"displayName":"Adam Majmudar","userId":"11411695890251280564"},"user_tz":420},"id":"AtknTNFEMswc","outputId":"104444c3-2bbd-4668-c4fb-59d580f576dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["length of dataset in characters 1115394\n","---\n"," First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n","Is't a verdict?\n","\n","All:\n","No more talking on't; let it be done: away, away!\n","\n","Second Citizen:\n","One word, good citizens.\n","\n","First Citizen:\n","We are accounted poor\n"]}],"source":["with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","print(\"length of dataset in characters\", len(text))\n","print(\"---\\n\", text[:500])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6frdy6RoMxfp"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLrGdvcLM0gY"},"outputs":[],"source":["# gpu\n","batch_size = 64\n","block_size = 256\n","max_iters = 5000\n","eval_interval = 500\n","learning_rate = 3e-4\n","device = torch.device(\"cuda\")\n","eval_iters = 200\n","n_embd = 384\n","n_head = 6\n","n_layer = 6\n","dropout = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkHyEn6KM4k0"},"outputs":[],"source":["torch.manual_seed(1337)\n","torch.set_default_device(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQECUZ6HM9X4"},"outputs":[],"source":["# get all the unique characters in the text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","\n","# create a mapping from characters to integers (tokenization)\n","stoi = { ch: i for i,ch in enumerate(chars) }\n","itos = { i: ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s]\n","decode = lambda l: ''.join([itos[i] for i in l])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zo_dSq_0M-D5"},"outputs":[],"source":["# get the encoded dataset\n","data = torch.tensor(encode(text), dtype=torch.long)\n","\n","# split into train and test split\n","n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ureR7zv6NAim"},"outputs":[],"source":["def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","\n","    # we create the training batches by offseting sequences, making the next word the y value for the previous word\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","\n","    x, y = x.to(device), y.to(device)\n","    return x, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9CCTyHt7NDN-"},"outputs":[],"source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, y = get_batch(split)\n","            # get losses from the model itself\n","            logits, loss = model(X, y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZubnhDNgNIJm"},"outputs":[],"source":["# a single attention head\n","class Head(nn.Module):\n","    def __init__(self, head_size):\n","        super().__init__()\n","\n","        # q, k, v\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","\n","        # used for masked attention (mask future tokens)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)\n","        q = self.query(x)\n","\n","        # QK^T/sqrt(dim)\n","        wei = k @ q.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","\n","        # softmax(QK^T/sqrt(dim))\n","        wei = F.softmax(wei, dim=-1)\n","        wei = self.dropout(wei)\n","\n","        # weighted aggregation of values\n","        v = self.value(x)\n","\n","        # full attention - Attention = softmax(qk^T/sqrt(dim))\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","# multi-headed attention, just concatenating the result of multiple heads\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        # store all the attention heads\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        # prokekection into the residual pathway\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # concatenate the result of all attention heads into one vector\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","# feed forward layers that come after multiheaded attention\n","class FeedForward(nn.Module):\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            # projection layer into the residual pathway\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        # each block has its own multi-headed attention + feed-forward\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedForward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        # add residuals to improve optimization\n","        # apply layernorms before going into modules\n","        x = x + self.sa(self.ln1(x)) # self-attention\n","        x = x + self.ffwd(self.ln2(x)) # feed-forward\n","        return x\n","\n","class GPT(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        # create the position embeddings for each position\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        # idx and targets are both (B,T) tensor of integers\n","        B, T = idx.shape\n","\n","        # embedding for each token\n","        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n","\n","        # positional embedding for each token\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n","\n","        # x has token embedding + position\n","        x = tok_emb + pos_emb # (B, T, C)\n","\n","        # apply all blocks w/ attention and feed forward layers\n","        x = self.blocks(x)\n","\n","        # get logits by using the final linear layer to predict the next token\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens (context window!)\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeM-vlcGNR1I"},"outputs":[],"source":["model = GPT()\n","m = model.to(device)\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":836020,"status":"ok","timestamp":1716097380105,"user":{"displayName":"Adam Majmudar","userId":"11411695890251280564"},"user_tz":420},"id":"sVlGA_wRNJUa","outputId":"568fca39-78c5-4f48-b73b-783cfed73ae1"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 4.5183, val loss 4.5049\n","step 500: train loss 1.7811, val loss 1.9220\n","step 1000: train loss 1.4491, val loss 1.6516\n","step 1500: train loss 1.3143, val loss 1.5616\n","step 2000: train loss 1.2365, val loss 1.5349\n","step 2500: train loss 1.1772, val loss 1.5225\n","step 3000: train loss 1.1199, val loss 1.5229\n","step 3500: train loss 1.0634, val loss 1.5410\n","step 4000: train loss 1.0079, val loss 1.5487\n","step 4500: train loss 0.9487, val loss 1.5795\n","\n","Since thy birt: till thy blood, sits and after the other\n","fires; then's the wind--though I fear, the staged what I\n","shall say, by need with transporturney of all,\n","the shepherd, a doth eten one thing\n","purchase to his temperant throw.\n","\n","MENENIUS:\n","What was ha sometime overed man? They say, my lords.\n","O unlike and leepend the good wall!\n","Hour the news several days. This mother day!\n","O montal, I'll peer it soject\n","To see him to-night.\n","\n","CORIOLANUS:\n","Where is the four weak poised art\n","To make so her worshipped l\n"]}],"source":["for iter in range(max_iters):\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"]},{"cell_type":"markdown","source":["Now with our model fully trained, we can generate some text from it - you can see that it's looking a lot better than the random characters it starts with (although of course, since the dataset size is limited and model size is limited, it isn't perfect)."],"metadata":{"id":"VDBLeh8Glgsq"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25112,"status":"ok","timestamp":1716097682465,"user":{"displayName":"Adam Majmudar","userId":"11411695890251280564"},"user_tz":420},"id":"sEv4YNRKSGNX","outputId":"38bb5b14-5ba4-48f9-c5a8-de94076c51a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","DORSET:\n","Readen truth that makes me levish of Bushy,\n","Shall I strawber in all this brare heir leap.\n","Then my part in her cheeks my wife unto your bed.\n","If Burgunderies her and cursed by lent\n","Hath upon me, and descend her brothers there were last,\n","Thy heads and frame to she diet, to see her petty ear.\n","Feeping this, see, her lords, all help all one:\n","We have been sole alraady, and I was before her\n","combardent, and by Romeo are the common taxe,\n","are the ruse-for fee-and every pine of services\n","That speeding days: in her purpose what bought were insatial\n","Marchans keeps. I have reasy\n","Come, or tell us plate, but stimate:\n","Only sins be here none beyed on you;\n","His idlen and resol majesty over yet:\n","The noble mother usages my head, my lord and\n","If consisted worms, and he may be, the laid was.\n","\n","QUEEN MARGARET:\n","Involt, daughter, nothing capame into made.\n","\n","QUEEN MARGARET:\n","Blind time, and lessen title peril thou fae.\n","\n","QUEEN MARGARET:\n","No, not In King Henry shall go.\n","To help them, and then take again.\n","I canst t\n"]}],"source":["context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}