{"cells":[{"cell_type":"markdown","metadata":{"id":"N0T1BTma2Adl"},"source":["# Explanation\n","\n","All state-of-the-art language models after BERT have depending on pre-training on internet-scale datasets to develop the majority of their knowledge. Because this pre-training is not conducive to any specific task, the fine-tuning stage is critical to make these models useful in production.\n","\n","The most effective assistant models, like the InstructGPT series, also depend heavily on fine-tuning to make the models behave like assistants and be helpful and aligned.\n","\n","Because of the importance of fine-tuning, efficient fine-tuning methods are important.\n","\n","Initially, fine-tuning was performed by training the base model on a new dataset and retuning all the parameters, which is quite expensive for large models.\n","\n","[Adapters](https://arxiv.org/abs/1902.00751) introduced a far more efficient approach to fine-tuning where new \"adapter\" layers were added inside each block of the transformer, and then only these layers were trained during fine-tuning.\n","\n","As a result, a much smaller number of parameters needed to be trained, making the fine-tuning stage much more efficient.\n","\n","However, by nature of introducing new parameters and non-linearities into the model, adapters can cause unpredictable shifts in the model behavior (as the original architecture's representation spaces were meant for the architecture without adapters).\n","\n","Because of this, the ideal fine-tuning method would be both _parameter efficient_ and also introduce _no new parameters_, so it couldn't hurt the existing quality of the model.\n","\n","**LoRA** is the solution to this challenge. It introduces a fine-tuning approach that's both parameter efficient, and fully compatible with existing architectures.\n","\n","### Intuition\n","\n","Instead of introducing new parameters to train, LoRA works by updating the existing parameters in the network.\n","\n","If we did this the standard way (by updating all the parameters during training), it would not achieve any efficiency over standard fine-tuning. But LoRA achieves fine-tuning of all the parameters in the network without training all the parameters directly.\n","\n","Instead, LoRA trains an equivalent lower dimension representation of the parameters in the network - this means that it has less parameters to train, but also less degrees of freedom to train on, meaning it can't make the same size of changes to the network as the pre-training stage.\n","\n","Empirically, it turns out that this fine-tuning method achieves high quality results despite the simplification, which suggests that the nature of changes made to models during the fine-tuning stage doesn't require modification of all the parameters.\n","\n","### Math\n","\n","For any weight matrix $W$, LoRA works by computing a small change to that weight matrix to produce an effective fine-tuned weight matrix $W_0$ via:\n","\n","$$W_0 = Wx + \\Delta Wx = Wx + BAx$$\n","\n","During fine-tuning, we fix the values of the original weight matrix $W$ and only modify the parameters of the $\\Delta W$ matrix. This is convenient so we can preserve the original pre-trained model, which is convenient during inference where we may want to switch out different fine-tunes of the same base model.\n","\n","Instead of modifying the parameters of $\\Delta W$ directly, which would result in training the same number of parameters as the original network, we instead train parameters from a low-rank decomposition of $\\Delta W$ via $BA$, which has a far smaller number of parameters to train.\n","\n","Importantly, the total number of parameters $B$ and $A$ combined are much smaller than the number of parameters in $\\Delta W$.\n","\n","This allows us to train a far smaller number of parameters, which is parameter efficient, while still being able to fine-tune the whole model.\n","\n","To make sure that the the fine-tuned matrix starts off close to the original pre-trained matrix, we usually initialize all the parameters in $B$ and $A$ to zero first, and then let them improve from there.\n","\n","This has an interesting implication - it appears that the modifications to a model made during fine-tuning are fundamentally \"low-rank\" modifications to the parameters - in other words, the parameters are updated in only a small subset of the dimensions.\n","\n","LoRA also explores how this transformation appears to amplify the effect of the model having learned to do certain tasks during pre-training, but not emphasizing this ability.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TFvMAkRH2Adm"},"source":["# My Notes\n","\n","## 📜 [Low-Rank Adaptation of Large Language-Models](https://arxiv.org/pdf/2106.09685)\n","\n","> As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible.\n","\n","> We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each\n","> layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.\n","\n","Seems to be building on adapters to make fine-tuning large models like GPT-3 feasible at scale.\n","\n","> We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA.\n","\n","> More importantly, these methods [of fine-tuning models by extending model depth or reducing the model’s usable sequence] often fail to match the fine-tuning baselines, posing a trade-off between efficiency and model quality.\n","\n","> We hypothesize that the change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed Low-Rank Adaptation (LoRA) approach.\n","\n","> LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers’ change during adaptation instead, while keeping the pre-trained weights frozen.\n","\n","LoRA lets the same base model be used for different tasks, makes training more efficient, introduces no inference latency, and can be combined with many previous optimization methods like prefix-tuning.\n","\n","### Problem Statement\n","\n","> One of the main drawbacks for full fine-tuning is that for _each_ downstream task, we learn a _different_ set of parameters $\\Delta \\Phi$ whose dimension $|\\Delta \\Phi|$ equals $|\\Phi_0|$. Thus, if the pre-trained model is large, storing and deploying many independent instances of fine-tuned models can be challenging, if at all feasible.\n","\n","> In the subsequent sections, we propose to use a low-rank representation to encode $\\Delta \\Phi$ that is both compute- and memory-efficient.\n","\n","### Aren’t Existing Solutions Good Enough?\n","\n","> There is no direct ways to bypass the extra compute in adapter layers.\n","\n","> We observe that prefix tuning is difficult to optimize and that its performance changes non-monotonically in trainable parameter.\n","\n","### Our Method\n","\n","**1. Low-Rank-Parameterized Update Matrices**\n","\n","> For a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$, we constrain its update by representing the latter with a low-rank de-composition $W_0 + \\Delta W = W_0 + BA$, where $B \\in \\mathbb{R}^{d \\times r}$, and the rank $r \\ll \\min(d, k)$.\n","\n","> For $h = W_0x$, our modified forward pass yields:\n","\n","$$\n","h = W_0x + \\Delta Wx = W_0x + BAx\n","$$\n","\n","Instead of optimizing a completely new set of parameters $\\Delta W$ with dimension $d \\times d$ in order to adapt the parameters of the original matrix $W_0$, we can instead create a low-rank decomposition of matrix $\\Delta W = BA$ where the dimensions of $B$ and $A$ are $d \\times r$ and $r \\times d$ respectively. Thus, if $r \\ll d$, this decomposition still yields a matrix of dimension $d \\times d$ while needing to optimize $2rd$ parameters instead of $d^2$ parameters, which is a massive optimization.\n","\n","> LoRA takes a step further and does not require the accumulated gradient update to weight matrices to have full-rank during adaptation.\n","\n","> In other words, as we increase the number of trainable parameters, training LoRA roughly converges to training the original model.\n","\n","> When deployed in production, we can explicitly compute and store $W = W_0 + BA$.\n","\n","> When we need to switch to another downstream task, we can recover $W_0$ by subtracting $BA$ and then adding a different $B'A'$, a quick operation with very little memory overhead.\n","\n","> Critically, this guarantees that we do not introduce any additional latency during inference compared to a fine-tuned model by construction.\n","\n","**2. Applying LoRA to Transformer**\n","\n","> In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the number of trainable parameters.\n","\n","> We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n","\n","> The most significant benefit comes from the reduction in memory and storage usage.\n","\n","### Understanding the Low-Rank Updates\n","\n","> (1) Given a parameter budget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt to maximize downstream performance?\n","> (2) Is the “optimal” adaptation matrix $\\Delta W$ _really rank-defficient?_ If so, what is a good rank to use in practice?\n","> (3) What is the connection between $\\Delta W$ and W? Does $\\Delta W$ highly correlate with W? How large is $\\Delta W$ comparing to W?\n","\n","**1. Which Weight Matrices in Transformer Should We Apply LoRA To?**\n","\n","![Screenshot 2024-05-16 at 1.55.14 PM.png](../../images/Screenshot_2024-05-16_at_1.55.14_PM.png)\n","\n","**2. What is the Optimal Rank $r$ for LoRA**\n","\n","![Screenshot 2024-05-16 at 1.56.08 PM.png](../../images/Screenshot_2024-05-16_at_1.56.08_PM.png)\n","\n","> We argue that increasing r does not cover a more meaningful subspace, which suggests that a low-rank adaptation matrix is sufficient.\n","\n","**3. How Does the Adaptation Matrix $\\Delta W$ Compare to W**\n","\n","> This suggests that the low-rank adaptation matrix potentially _amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model._\n","\n","This is the core intuition behind why LoRA works. It’s an attempt at understanding why the transformation done by fine-tuning is inherently low rank.\n","\n","In practice, when looking at the SVD of $W$, it appears that LoRA’s effect is to amplify the directions that are not already emphasized in $W$, potentially augmenting existing representations that already existed in $W$ which are particularly relevant to specific tasks but not emphasized in the original matrix.\n","\n","### Conclusion\n","\n","> We propose LoRA, an efficient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality. Importantly, it allows for quick task-switching when deployed as a service by sharing the vast majority of the model parameters.\n","\n","> While we focused on Transformer language models, the proposed principles are generally applicable to any neural networks with dense layers.\n","\n","This discovery is actually generally valuable for all fine-tuning and transfer learning cases with neural networks.\n"]},{"cell_type":"markdown","source":["# Implementation\n","\n","I've attempted to create a toy example here just to see how LoRA works in code (it's not plugged into an actual model, but I mainly want to demonstrate what the decomposition actually looks like in practice in terms of trainable parameters).\n","\n","The example below is my attempt at a super simple version of some of the modules in the [official LoRA implementation](https://github.com/microsoft/LoRA/tree/main)."],"metadata":{"id":"FPaXnHPi7r_-"}},{"cell_type":"code","source":["# This simple class is just used to store some meta data about any layer that uses LoRA\n","class LoRA():\n","    def __init__(self, r, merge_weights):\n","        # r specifies the rank of this adaptation - if r is set to 0, then lora wont do anything\n","        self.r = r\n","        self.merged = False\n","        self.merge_weights = merge_weights\n","\n","class Linear(nn.Linear, LoRA):\n","    # LoRA implemented in a dense layer\n","    def __init__(self, in_dim out_dim, r=0, merge_weights=True):\n","        nn.Linear.__init__(self, in_dim, out_dim)\n","        LoRALayer.__init__(self, r, merge_weights)\n","\n","        # Assuming LoRA is active, we create the trainable B and A matrices\n","        if r > 0:\n","            # Importantly, the inner dimension of the matrices are determined by rank \"r\"\n","            # And the in_features, out_features beating the outer dimensions mean BA multiply to be\n","            # the correct dimension of the original weight matrix\n","            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n","            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n","            # Freezing the pre-trained weight matrix, and we only update A and B during training\n","            self.weight.requires_grad = False\n","\n","\n","    def train(self, mode = True):\n","        nn.Linear.train(self, mode)\n","        if mode:\n","            # During training mode, we make sure that the weights are not merged into the\n","            # original weight matrix\n","            if self.merge_weights and self.merged:\n","                if self.r > 0:\n","                    self.weight.data -= self.lora_B @ self.lora_A\n","                self.merged = False\n","        else:\n","            # Turning testing, we merge in BA into W to make the fine-tuned weight matrix\n","            if self.merge_weights and not self.merged:\n","                if self.r > 0:\n","                    self.weight.data += self.lora_B @ self.lora_A\n","                self.merged = True\n","\n","    def forward(self, x):\n","        if self.r > 0 and not self.merged:\n","            # During feed-forward, we first compute the original result of weights W\n","            result = F.linear(x, self.weight, bias=self.bias)\n","            # And then we add on the small changes induced by the LoRA BA matrix after\n","            result += (self.lora_A @ self.lora_B.transpose(0, 1))\n","            return result\n","        else:\n","            return F.linear(x, self.weight, bias=self.bias)"],"metadata":{"id":"fZONIfyF78xf"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}