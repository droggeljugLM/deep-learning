{"cells":[{"cell_type":"markdown","source":["# Explanation\n","\n","The variation auto-encoder (VAE) is designed to learn complex distributions with continuous variables that are subject to a lot of variations. Images are a perfect example of this - each pixel is represented by the continuous RGB values where each has $256^3$ possibilities, and each image can maintain a representation of the same object while individual pixels may be moved around or changed in value in a huge number of ways.\n","\n","The VAE introduces a \"stochastic variational inference\" algorithm that can effectively model such complex distributions by incorporating probability and randomness directly into the model itself (rather than the prior deterministic structure of most neural networks).\n","\n","The VQ VAE then introduces a discrete algorithm (that doesn't use randomness) inspired by the VAE that is effective at modeling similar distributions.\n","\n","The common intuition shared between the VAE & the VQ VAE is that in highly complex distributions (like the one that creates images), reconstructing the data involves learning the lower dimensionality representations that encode the most important features (called the \"modes\" of the data), and then learning to add back into the complexity of the details on top of these features through some means.\n","\n","### VAE\n","\n","The VAE approaches this problem with an encoder-decoder architecture that forces the model to learn to separate out the high-level features from the complexity.\n","\n","Specifically, it learns to pick out the high-level features explicitly, and then approximate the complexity with random sampling.\n","\n","In probability, the aggregation of a large number of random variables often approximates to some far simpler distribution (usually the Gaussian distribution). This is the intuition behind the VAE.\n","\n","Instead of trying to learn the complexity directly, the VAE learns to pick out features by compressing data into two representation vectors: the $\\mu$ and $\\sigma$ vectors which represent the mean and variance of a Gaussian distribution. Then, in order to actually synthesize specific images, it samples from this distribution (adding back complexity).\n","\n","The VAE consists of two parts:\n","\n","1. The encoder, which produces the $\\mu$ and $\\sigma$ vectors for each input value (which should correspond with the high-level features)\n","2. The decoder, which samples from a Gaussian distribution with the $\\mu$ and $\\sigma$ values, and then uses this sampled vector to recreate the image.\n","\n","The overall model is trained my trying to maximize the likelihood that the decoder reproduces the image from the vector sampled from encoders representation, forcing the encoder to push it's encoded vectors closer to the modes of the dataset, and allowing the random sampling to approximate the complexity.\n","\n","Importantly, adding randomness between the encoder and decoder introduces a non-differentiable step that would cut off the gradient flow from the decoder back through to the encoder. The VAE solves this using the **reparameterization trick**, which isolates the randomness term by sampling vectors via $z = \\mu + \\sigma \\cdot \\epsilon$, where gradient can still flow back through $\\sigma$ and $\\mu$, and the randomness is isolated to $\\epsilon$ which can be independently sampled, rather than making $z$ just directly sample from a Gaussian distribution.\n","\n","### VQ VAE\n","\n","The VQ VAE still effectively learns to separately model the modes and complexity in its training distribution, but it no longer uses randomness directly in the model.\n","\n","Instead, the VQ VAE encoder outputs vectors which are then \"quantized\" to fit into a finite codebook of $K$ vectors before passing to the decoder. In other words, the VQ VAE maintains a list of a finite number of vectors, and for each vector outputted by the decoder, it replaces that vector with the nearest vector in the codebook before passing it to the decoder.\n","\n","Through the training process, the VQ VAE updates this codebook to make it more effective for the decoder to reproduce the images.\n","\n","In this way, the VQ VAE is actually trianing 3 separate parts of the model:\n","1. The decoder has to use the codebook to effectively reproduce images\n","2. The codebook has to update its embedding vectors to be most useful to the decoder (and most representative of the images)\n","3. The encoder has to encode vectors to be close to their most representative forms in the code book.\n","\n","This process forces the VQ VAE to learn discrete representations of the inputs, which forces it to encode the most essential information.\n","\n","Because of the quantization process, there's a \"quantization-error\" that comes from some of the complexity being removed by the model due to snapping the vectors produced by the encoder directly into codebok vectors. This forces the decoder to learn to add back the complexity in the data based on lower-dimensional representations from the codebook.\n","\n","During training, the model has to make the encoder produce better representations, and make the decoder produce images from the quantized representations, but it also has to optimize the codebook. These concerns are represented in the loss function:\n","\n","$$\n","L = \\log p(x|z_q(x)) + || \\textrm{sg}[z_e(x)] - e ||_2^2 | - \\beta || z_e(x) - sg[e] ||_2^2\n","$$\n","\n","The left most term here represents the objective for the encoder and decoder to work together to reconstruct the image (the standard task of the auto-encoder).\n","\n","The second term minimizes the distance between the encoders outputs and the embeddings in the codebook, which is what actually improves the utility of the codebook.\n","\n","The third term can be thought of as a \"commmitment loss\" which forces the encoder to move its encoded vectors close to the codebook vectors, preventing these 2 from optimizing separately and diverging.\n","\n","The VQ VAE proves effective at reconstructing images, meeting the quality of GANs - and it was also the generative model of choice for OpenAI's original DALL E model.\n","\n","\n","\n"],"metadata":{"id":"GEIPT9wz2Pin"}},{"cell_type":"markdown","metadata":{"id":"qJYwYI0T1uGz"},"source":["# My Notes\n","\n","## 📜 [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114)\n","\n","> How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets?\n","\n","How do we make probabilistic models (where parts of the feed-forward process come from sampling from a distribution, rather than all being deterministic) on large datasets where the apparent data generating distribution is continuous an intractable?\n","\n","An example of this is large image datasets, where the actual distribution producing each image is extremely complex, as lots of noise is permitted and image content will still remain the same (this is an indicator of the complexity of the distribution - you have to account for so many possibilities, which are all continuous in terms of pixel values and positions).\n","\n","> We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case.\n","\n","**variational inference algorithm** - an algorithm that models the true (intractable) distribution of the data with a much simpler variational distribution, like the Gaussian distribution, which allows simpler inference\n","\n","Importantly, modeling the true distribution with a simpler and differentiable distributions means we can actually compute gradients\n","\n","> We show that a re-parameterization of the variational lower bound\n","> yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods.\n","\n","One of the most important contributions of the paper - they provide a way to alter the random sampling component (from a Gaussian) usually present in the variational lower bound optimization (explained later) that makes the random sampling actually differentiable.\n","\n","This is critical - this innovation makes it actually possible to use stochastic gradient descent and deep neural networks with back-propagation to optimize probabilistic models.\n","\n","> Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.\n","\n","Another key innovation - they show that you can model the extremely complex and unknown posterior distribution with a separate internal distribution in the model, which can be tuned, and then used to run inference.\n","\n","This recognition model can be optimized using the lower bound estimator.\n","\n","> The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior.\n","\n","The inspiration behind the VAE approach from statistics. The variational Bayesian approach, which models a more complex distribution with a simpler one (like the Gaussian).\n","\n","> Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case.\n","\n","The usual approach to VB is the “mean-field” approach which involves calculating an expectation of conditional probabilities based on random variables sampled from a distribution - and as a result often involves integrating over CDFs which takes “analytical solutions” that are often intractable.\n","\n","> We show how a re-parameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.\n","\n","They introduce a new method that updates on this VB method to make it usable in stochastic gradient descent by addressing the problems brought up above.\n","\n","> We propose the AutoEncoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling\n","\n","They create a joint model where one part learns to form a recognition model that approximates the more complex data generating distribution, and then an inference model that samples from this distribution to reproduce the original values.\n","\n","> Which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint.\n","\n","This architecture with both a recognition model and an inference model forces the recognition model to encode useful representations that can be reinterpreted back to the original data.\n","\n","This method is far more efficient than previous methods that require long chains of compute like Markov Chain Monte Carlo to accomplish the same thing.\n","\n","> When a neural network is used for the recognition model, we arrive at the variational auto-encoder.\n","\n","### Method\n","\n","> We will restrict ourselves […] to […] where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables.\n","\n","**1. Problem Scenario**\n","\n","We deal with situations where we have some dataset $X = \\{ x^{(i)} \\}_{i=1}^N$ that consists of a process of values where (1) some values $z^{(i)}$ are generated from some distribution $p_{\\theta^*}(z)$ and (2) the values $x^{(i)}$ are drawn from $p_{\\theta^*}(x|z)$, and that their PDFs are tractable, but that calculating the integral of the marginal likelihood $\\int p_\\theta(z) p\\theta(x|z) dz$ (necessary to calculate the evidence lower bound) is intractable.\n","\n","> We are interested in, and propose a solution to, three related problems in the above scenario:\n","\n","1. Efficient approximate ML or MAP estimation for the parameters $\\theta$. The parameters […] allow us to mimic the hidden random process and generate artifical data that resembles the real data.\n","\n","2. Efficient approximate posterior inference of the latent variable $z$ given an observed value $x$ for a choice of parameters $\\theta$. This is useful for coding or data representation tasks.\n","\n","3. Efficient approximate marginal inference of the variable $x$. This allows us to perform all kinds of inference tasks where a prior over $x$ is required. Common applications in computer vision include image denoising, in-painting and super-resolution.\n","   >\n","\n","Each of these 3 objectives provides real value that will become important. The representation space created by the encoder becomes very valuable (and can even be used for embeddings).\n","\n","Additionally, the inference part can be used for many image generation and manipulation applications.\n","\n","We introduce a “recognition model” $q_\\phi(z|x)$ meant to model the true posterior distribution, which will be called the _encoder_. Then, the distribution $p_\\theta(x|z)$ will be called the _decoder_.\n","\n","**1. The Variational Bound**\n","\n","> The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints $\\log p_\\theta(x^{(1)}, …, x^{(N)}) = \\sum_{i=1}^N \\log p_\\theta(x^{(i)})$, which can be rewritten as:\n","\n","$$\n","\\log p_\\theta(x^{(i)}) = D_{KL}(q_\\phi(z|x^{(i)}), p_\\theta(z|x^{(i)})) + \\mathcal{L}(\\theta, \\phi; x^{(i)})\n","$$\n","\n","The KL divergence models the difference between our approximate distribution and the true distribution of the latent variables $z$ given the actual sampled value $x^{(i)}$.\n","\n","The second term $\\mathcal{L}$, known as the _variational lower bound_, marks the lower bound of the probability of any observed data-point appearing in our distribution:\n","\n","$$\n","\\log p_\\theta(x^{(i)}) \\geq \\mathcal{L}(\\theta, \\phi; x^{(i)}) = \\mathbb{E}_{q_\\phi(z|x)}[-\\log q_\\phi(z|x) + \\log p_\\theta(x,z)]\n","$$\n","\n","This term represents the evidence lower bound. We take the expectation of values sampled over our approximate distribution $q_\\phi$ for all values $z$ sampled given $x$, weighted by their relative probabilities.\n","\n","For each, we want to maximize the first term (the entropy) $- \\log q_\\phi(z|x)$, which means minimizing the probability of $z$ being sampled given $x$ generally. This spreads out the distribution more, preventing $z$ from concentrating around too few values.\n","\n","The second term $\\log p_\\theta(x,z)$ ensures that our model learns useful representations for $z$ that allow the decoder to actually reconstruct $x$.\n","\n","These two terms work together to make the ELBO effective. The first term can be thought of as saying “explore a broad range of possibilities for $z$ for each $x$, instead of narrowing down” forcing the distribution to be very spread out (different $z$ values should not be highly used across the model).\n","\n","Meanwhile, the second term is saying “focus on the values of $z$ that effectively help the decoder to recreate $x$.”\n","\n","We can then rewrite the above equation for the ELBO using the KL divergence as follows:\n","\n","$$\n","\\mathcal{L}(\\theta, \\phi; x^{(i)}) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(z) + \\log p_\\theta(x|z) - \\log q_\\phi(z|x)] \\\\\n","D_{KL}(q_\\phi(z|x), p_\\theta(z)) = \\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\theta(z|x) - \\log p_\\theta(z)] \\\\\n","\\mathcal{L}(\\theta, \\phi; x^{(i)}) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x), p_\\theta(z))\n","$$\n","\n","Thus, the first term of this maximizes the probability that our decoder can produce $x$ from the $z$ values sampled from the encoder, and the second term minimizes the difference between the approximate distributions condition $z|x$ and the overall sampling of $z$.\n","\n","**3. The SGVB Estimator and AEVB Algorithm**\n","\n","> In this section we introduce a practical estimator of the lower bound and its derivates w.r.t the parameters.\n","\n","Here, we address the ability to model our evidence lower bound for maximization with an efficient estimator.\n","\n","> Under certain mild conditions […] for a chosen approximate posterior $q_\\phi (z|x)$ we can reparameterize random variable $\\hat{z} \\sim q_\\phi(z|x)$ using a differentiable transformation $g_\\phi(\\epsilon, x)$ of an (auxiliary) noise variable $\\epsilon$\n","\n","$$\n","\\hat{z} = g_\\phi(\\epsilon, x) \\textrm{ with } \\epsilon \\sim p(\\epsilon) \\\\\n","E_{q_\\phi(z|x^{(i)})}[f(z)] = E_{p(\\epsilon)}[f(g_\\phi(\\epsilon, x^{(i)}))] \\simeq \\frac{1}{L} \\sum_{l=1}^L f(g_\\phi(\\epsilon^{(l)}, x^{(i)})) \\\\\n","$$\n","\n","Here, we create a re-parameterized estimator introducing some random noise through the variable $\\epsilon$ which introduces noise. Then, you can take the expectation by taking the average over a number of samples influenced by this random noise, rather than calculating the integral through analytical means.\n","\n","We can apply this sampling method to our calculation of $\\mathcal{L}$ to create an estimator function $\\mathcal{L}^A(\\theta, \\phi; x)$ that can be used to optimize the ELBO.\n","\n","> The KL-divergence term can then be interpreted as regularizing $\\phi$, encouraging the approximate posterior to be close to the prior $p_\\theta(z)$.\n","\n","> A connection with auto-encoders becomes clear when looking at the objective function. The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error.\n","\n","**4. The re-parameterization trick**\n","\n","> The essential parameterization trick is quite simple. Let $z$ be a continuos random variable, and $z \\sim q_\\phi(z|x)$ be some conditional distribution. It is then often possible to express the random variable $z$ as a deterministic variable $z = g_\\phi(\\epsilon, x)$ where $\\epsilon$ is an auxiliary variable with independent marginal $p(\\epsilon)$ and $g_\\phi(.)$ is some vector-valued function parameterized by $\\phi$.\n","\n","This reframing (re-parameterization) allows us to make the sampling of the random variable $z$ differentiable (via the parameters $\\phi$) by isolating the noise to a separate variable $\\epsilon$.\n","\n","> Take, for example, the univariate Gaussian case: let $z \\sim p(x|x) = \\mathcal{N}(\\mu, \\sigma^2)$. In this case, a valid re-parameterization is $z = \\mu + \\sigma\\epsilon$, where $\\epsilon$ is an auxiliary noise variable $\\epsilon \\sim \\mathcal{N}(0,1)$.\n","\n","### Experiments\n","\n","> We trained generative models of images from the MNIST and Frey Face datasets and compared learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.\n","\n","![Screenshot 2024-05-18 at 11.58.38 AM.png](../../images/Screenshot_2024-05-18_at_11.58.38_AM.png)\n","\n","### Conclusion\n","\n","> We have introduce a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables.\n","\n","> The theoretical advantages are reflected in experimental results.\n"]},{"cell_type":"markdown","metadata":{"id":"ISKOm6Ka1uG1"},"source":["## 📜 [Neural Discrete Representation Learning](https://arxiv.org/pdf/1711.00937)\n","\n","> In this paper, we propose a simple yet powerful generative model that learns […] discrete representations [without supervision].\n","\n","> Our model, the Vector Quantized Variational Auto Encoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static.\n","\n","> Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.\n","\n","> Our goal is to achieve a model that conserves the important features of the data in its latent space while optimizing for maximum likelihood.\n","\n","> In this paper, we argue for learning discrete and useful latent variables, which we demonstrate on a variety of domains.\n","\n","> Learning representations with continuous features have been the focus of many previous work, however we concentrate on discrete representations which are potentially a more natural fit for many of the modalities we are interested in.\n","\n","They observe that many data distributions including text, and even images given that they can be described by text, can be represented in discrete ways.\n","\n","> Our model, which relies on vector quantization (VQ), is simple to train, does not suffer from large variance, and avoids the “posterior collapse” issue which has been problematic with many VAE models that have a powerful decoder, often caused by latents being ignored.\n","\n","### VQ-VAE\n","\n","> VAEs consist of the following parts: an encoder network which parameterizes a posterior distribution $q(z|x)$ of discrete latent random variables $z$ given the input data $x$, a prior distribution $p(z)$, and a decoder with a distribution $p(x|z)$ over input data.\n","\n","**1. Discrete Latent Variables**\n","\n","> We define a latent embedding space $e \\in R^{K \\times D}$ where $K$ is the size of the discrete latent space, and $D$ is the dimensionality of each latent embedding vector $e_i$\n","\n","This model uses an encoder producing an output $z_e(x)$ for each input $x$.\n","\n","Then, this value $z_e(x)$ is passed through a posterior categoorical distribution that collapses the output value into 1-of-K embedding vectors\n","\n","$$\n","q(z = k|x) = \\begin{cases}\n","  1 & \\textrm{for } k = \\textrm{argmin}_j || z_e(x) - e_j ||_2 \\\\\n","  0 & \\textrm{otherwise}\n","\\end{cases}\n","$$\n","\n","> The representation $z_e(x)$ is passed through a discretisation bottleneck followed by mapping onto the nearest element of embedding $e$.\n","\n","**2. Learning**\n","\n","Since $q(z)$ has no gradient, we just copy over the gradient from decoder input $z_q(x)$ to encoder output $z_e(x)$.\n","\n","> Due to the straight-through gradient estimation of mapping from $z_e(x)$ to $z_q(x)$, the embeddings $e_i$ receive no gradients from the reconstruction loss $\\log p(z|z_q(x))$. Therefore, in order to learn the embedding space, we use one of the simplest dictionary learning algorithms, Vector Quantization (VQ).\n","\n","> The VQ objective uses the $l_2$ error to move the embedding vectors $e_i$ towards the encoder outputs $z_e(x)$.\n","\n","The actual embedding space is disconnected from the main gradient flow directly, and instead is just built to minimize the overall distance between the embedding vectors and the actual encoder outputs (maximizing the utility of each embedding to matching a variety of the outputs).\n","\n","> To make sure the encoder commits to an embedding and its output does not grow, we add a commitment loss, the third term in the [loss equation]. Thus, the total training objective becomes:\n","\n","$$\n","L = \\log p(x|z_q(x)) + || \\textrm{sg}[z_e(x)] - e ||_2^2 | - \\beta || z_e(x) - sg[e] ||_2^2\n","$$\n","\n","> where _sg_ stands for the stop-gradient operator that is defined as identity at forward computation time and has zero partial derivatives, thus effectively constraining its operand to be a non-updated constant.\n","\n","The first term (familiar from VAEs) tries to maximize the probability that $x$ is regenerated given the latents (embeddings) created by $z_q(x) = q(z_e(x))$ from the encoder and categorization.\n","\n","The second term minimizes the $L_2$ distance between the embedding vectors and the encoder outputs.\n","\n","The third term ensures that the encoder commits to its choice of embeddings and moves its encoded outputs closer to them so that the encoder outputs don’t slowly start to diverge from the embedding choices.\n","\n","**3. Prior**\n","\n","> The prior distribution over the discrete latents $p(z)$ is a categorical distribution, and can be mad autoregressive by depending on other $z$ in the feature map. Whilst training the VQ-VAE, the prior is kept constant and uniform.\n","\n","> After training, we fit an autoregressive distribution over $z$, $p(z)$, so that we can generate $x$ via ancestral sampling.\n","\n","### Experiments\n","\n","**1. Comparison with Continuous Variables**\n","\n","> Our model is the first among those using discrete latent variables which challenges the performance of continuous VAEs.\n","\n","**2. Images**\n","\n","> Images contain a lot of redundant information as most of the pixels are correlated and noisy, therefore learning models at the pixel level could be wasteful.\n","\n","> In this experiment we show that we can model $x = 128 \\times 128 \\times 3$ images by compressing them to a $z = 32 \\times 32 \\times 1$ discrete space (with $K=512$) via a purely de-convolutional $p(x|z)$.\n","\n","> We model images by learning a powerful prior (PixelCNN) over $z$.\n","\n","![Screenshot 2024-05-18 at 12.46.00 PM.png](../../images/Screenshot_2024-05-18_at_12.46.00_PM.png)\n","\n","![Screenshot 2024-05-18 at 12.46.35 PM.png](../../images/Screenshot_2024-05-18_at_12.46.35_PM.png)\n","\n","**3. Audio**\n","\n","> In all our audio experiments, we train a VQ-VAE that has a dilated convolutional architecture similar to WaveNet decoder.\n","\n","> This means that the VQ-VAE has, without any form of linguistic supervision, learned a high-level abstract space that is invariant to low-level features and only encodes the content of the speech.\n","\n","**4. Video**\n","\n","> It can be seen that the model has learnt to successfully generate a sequence of frames conditioned on given action without any degradation in the visual quality whilst keeping the local geometry correct.\n","\n","![Screenshot 2024-05-18 at 12.51.34 PM.png](../../images/Screenshot_2024-05-18_at_12.51.34_PM.png)\n","\n","### Conclusion\n","\n","> In this work we have introduced VQ-VAE, a new family of models that combine VAEs with vector quantization to obtain a discrete latent representation.\n","\n","> We have shown that VQ-VAEs are capable of modeling very long term dependencies through their compressed discrete latent space which we have demonstrated by generating 128 × 128 color images, sampling action conditional video sequences and finally using audio where even an unconditional model can generate surprisingly meaningful chunks of speech and doing speaker conversion.\n","\n","> All these experiments demonstrated that the discrete latent space learnt by VQ-VAEs capture important features of the data in a completely unsupervised manner.\n"]},{"cell_type":"markdown","metadata":{"id":"RIW-oWBM1uG1"},"source":["### 📜 [Generating Diverse High-Fidelity Images with VQ-VAE 2](https://arxiv.org/pdf/1906.00446)\n","\n","> We scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before.\n","\n","> We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN’s known shortcomings such as mode collapse and lack of diversity\n","\n","> It is well known that samples from [GANs] do not fully capture the diversity of the true distribution.\n","\n","> In contrast, likelihood based methods optimize negative log-likelihood (NLL) of the training data. This objective allows model-comparison and measuring generalization to unseen data.\n","\n","> In this paper we use ideas from lossy compression to relieve the generative model from modeling negligible information.\n","\n","### Background\n","\n","**1. Vector Quantized Variational Auto Encoder**\n","\n","> The VQ-VAE model can be better understood as a communication system. It comprises of an encoder that maps observations onto a sequence of discrete latent variables, and a decoder that reconstructs the observations from these discrete variables. Both encoder and decoder use a shared codebook.\n","\n","The VQ-VAE has a _codebook_ that it uses to communicate between the encoder and the decoder.\n","\n","The decoder maps the received indices of vectors in the codebook and uses it to reconstruct the original data via non-linearities. This is the “regeneration loss.”\n","\n","In addition, the VQ-VAE has _codebook loss_ to make the codebook match more closely with the encoder outputs, and the _commitment loss_ to encourage the output of the decoder to stay closer to the codebook.\n","\n","$$\n","\\mathcal{L}(x, D(e)) = ||x - D(e)||_2^2 + ||sg[E(x)] - e||_2^2 + \\beta || sg[e] - E(x) ||_2^2\n","$$\n","\n","**2. PixelCNN Family of Autoregressive Models**\n","\n","> Deep autoregressive models are common probabilistic models that achieve state of the art results in density estimation across several data modalities.\n","\n","### Method\n","\n","> The proposed method follows a two-stage approach: first, we train a hierarchical VQ-VAE to encode images onto a discrete latent space, and then we fit a powerful PixelCNN prior over the discrete latent space induced by all the data.\n","\n","This sets the intuition for transformer based image generation with VQ-VAEs as well - the place where PixelCNN is operating now can be replaced with a transformer with self-attention to learn the distribution.\n","\n","![Screenshot 2024-05-18 at 1.28.31 PM.png](../../images/Screenshot_2024-05-18_at_1.28.31_PM.png)\n","\n","**1. Stage 1: Learning Hierarchical Latent Codes**\n","\n","> As opposed to vanilla VQ-VAE, in this work we use a hierarchy of vector quantized codes to model large images. The main motivation behind this is to model local information, such as texture, separately from global information such as shape and geometry of objects.\n","\n","> The prior model over each level can thus be tailored to capture the specific correlations that exist in that level.\n","\n","> The structure of our multi-scale hierarchical encoder [has] a top latent code which models global information, and a bottom latent code, conditioned on the top latent, responsible for representing local details.\n","\n","**2. Stage 2: Learning Priors over Latent Codes**\n","\n","> In order to further compress the image, and to be able to sample from the model learned during, we learn a prior over the latent codes\n","\n","This separate model takes the embedding space learned by the auto-encoder and learns a prior on it.\n","\n","In this way, the auto-encoder has done the job of compressing the data in a way to get rid of less important information where it’s encoded outputs only represent important data to recreate the image.\n","\n","Then, when we train a neural network to learn the prior on the encoders output distribution, it’s effectively modeling the actual data generating distribution much more efficiently than if it were observing the original data since all the noise has been removed and only important features remain.\n","\n","This makes learning the prior distribution a powerful way to sample other points in the original state space that actually correspond with likely values.\n","\n","> From an information theoretic point of view, the process of fitting a prior to the learned posterior can be considered as lossless compression of the latent space by re-encoding the latent variables with a distribution that is a better approximation of their true distribution, and thus results in bit rates closer to Shannon’s entropy.\n","\n","The auto-encoder provides compression to a new distribution that can be modeled more effectively than the original due to the removal of noise.\n","\n","> In the VQ-VAE framework, this auxiliary prior is modeled with a powerful, autoregressive neural network such as PixelCNN in a post-hoc, second stage.\n","\n","**3. Trading off Diversity with Classifier Based Rejection Sampling**\n","\n","> Unlike GANs, probabilistic models trained with the maximum likelihood objective are forced to model all of the training data distribution.\n","\n","Because of this, having samples in the dataset that don’t match nicely with the proper underlying distribution adds a considerable challenge in actually converging to the correct data distribution.\n","\n","To mitigate this, they create an automated way to classify the quality of the samples.\n","\n","> In this work, we also propose an automated method for trading off diversity and quality of samples based on the intuition that the closer our samples are to the true data manifold, the more likely they\n","> are classified to the correct class labels by a pre-trained classifier.\n","\n","Using this method, they can classify the quality of the samples by their proximity to the underlying distribution.\n","\n","### Experiments\n","\n","> In this section, we present quantitative and qualitative results of our model trained on ImageNet 256 × 256.\n","\n","![Screenshot 2024-05-18 at 1.46.40 PM.png](../../images/Screenshot_2024-05-18_at_1.46.40_PM.png)\n","\n","![Screenshot 2024-05-18 at 1.48.12 PM.png](../../images/Screenshot_2024-05-18_at_1.48.12_PM.png)\n","\n","**1. Modeling High-Resolution Face Images**\n","\n","> Although modeling faces is generally considered less difficult compared to ImageNet, at such a high resolution there are also unique modeling challenges that can probe generative models in interesting ways. For example, the symmetries that exist in faces require models capable of capturing long range dependencies.\n","\n","### Conclusion\n","\n","> We propose a simple method for generating diverse high resolution images using VQ-VAE with a powerful autoregressive model as prior.\n","\n","> Our encoder and decoder architectures are kept simple and light-weight as in the original VQ-VAE, with the only difference that we use a hierarchical multi-scale latent maps for increased resolution.\n","\n","> We believe our experiments vindicate autoregressive modeling in the latent space as a simple and effective objective for learning large scale generative models.\n"]},{"cell_type":"markdown","source":["# Implementation\n","\n","Below is my implementation of a simple VAE that learns to reconstruct images with the MNIST dataset, based on this [great replication of the VAE and VQ VAE papers by Jackson Kang](https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/tree/master)."],"metadata":{"id":"Oo0G5ZSDCd5y"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np"],"metadata":{"id":"17UHnsSYBpRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path=\"~/datasets\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","data_dim = 784 # Each image is 28 x 28 = 784 pixels\n","batch_size = 100\n","hidden_dim = 400\n","latent_dim = 200\n","learning_rate = 1e-3\n","epochs = 30\n"],"metadata":{"id":"yI6mQmRZClGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.datasets import MNIST\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# MNIST\n","mnist_transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","kwargs = {'num_workers': 1, 'pin_memory': True}\n","\n","train_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n","test_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n","test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, **kwargs)"],"metadata":{"id":"K0cS0TaQDFyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim):\n","        super(Encoder, self).__init__()\n","\n","        # Hidden layers used to process the inputs before converting to latents\n","        self.hidden = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","        # Latent representations encoded into mean and log variance vectors\n","        self.mean = nn.Linear(hidden_dim, latent_dim)\n","        self.log_variance = nn.Linear(hidden_dim, latent_dim)\n","\n","        self.training = True\n","\n","    def forward(self, x):\n","        hidden = self.hidden(x)\n","        mean = self.mean(hidden)\n","        log_variance = self.log_variance(hidden)\n","\n","        return mean, log_variance"],"metadata":{"id":"irCfeiHKDiA8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim, output_dim):\n","        super(Decoder, self).__init__()\n","\n","        # Convert back from the latents to the reconstruction\n","        self.hidden = nn.Sequential(\n","            nn.Linear(latent_dim, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(hidden_dim, output_dim)\n","        )\n","\n","    def forward(self, x):\n","        x_hat = torch.sigmoid(self.hidden(x))\n","        return x_hat"],"metadata":{"id":"HJTyLSI_EtuU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self, Encoder, Decoder):\n","        super(Model, self).__init__()\n","        self.Encoder = Encoder\n","        self.Decoder = Decoder\n","\n","    def reparameterization(self, mean, variance):\n","        # Separate out the randomness into the epsilon term\n","        epsilon = torch.randn_like(variance).to(device)\n","\n","        # Now gradients can flow back through mean and variance stil\n","        z = mean + variance * epsilon\n","\n","        return z\n","\n","    def forward(self, x):\n","        mean, log_variance = self.Encoder(x)\n","\n","        # Use the reparameterization trick to keep randomness differentiable\n","        z = self.reparameterization(mean, torch.exp(0.5 * log_variance))\n","\n","        x_hat = self.Decoder(z)\n","        return x_hat, mean, log_variance"],"metadata":{"id":"QiEkFfY-EvPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder = Encoder(input_dim=data_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n","decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim = data_dim)\n","\n","model = Model(Encoder=encoder, Decoder=decoder).to(device)"],"metadata":{"id":"6yt1i8X1EyoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim import Adam\n","\n","def bce_loss(x, x_hat, mean, log_variance):\n","    # reconstruction loss encourages latents to model distribution better\n","    reconstruction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n","\n","    # kl div penalizes latents from deviating too far from gaussian\n","    kl_divergence = - 0.5 * torch.sum(1 + log_variance - mean.pow(2) - log_variance.exp())\n","\n","    # both balance each other out to make a good approximation\n","    return reconstruction_loss + kl_divergence\n","\n","optimizer = Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"LRx8Yt8AFHXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.train()\n","\n","for epoch in range(epochs):\n","    overall_loss = 0\n","    for batch_idx, (x, _) in enumerate(train_loader):\n","        x = x.view(batch_size, data_dim)\n","        x = x.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        x_hat, mean, log_variance = model(x)\n","        loss = bce_loss(x, x_hat, mean, log_variance)\n","\n","        overall_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f\"epoch {epoch + 1}: average loss {overall_loss / (batch_idx*batch_size)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqM10om7FKte","executionInfo":{"status":"ok","timestamp":1716414747113,"user_tz":420,"elapsed":207379,"user":{"displayName":"Adam Majmudar","userId":"11411695890251280564"}},"outputId":"618de1f2-4978-4ff0-ae92-7b211be03b00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1: average loss 172.74887594232575\n","epoch 2: average loss 128.2209591995774\n","epoch 3: average loss 117.5232250365192\n","epoch 4: average loss 113.37799142777024\n","epoch 5: average loss 110.49296118530884\n","epoch 6: average loss 108.71922556474333\n","epoch 7: average loss 107.27195219571682\n","epoch 8: average loss 106.2627855833942\n","epoch 9: average loss 105.38990369691673\n","epoch 10: average loss 104.8008842700073\n","epoch 11: average loss 104.23132069073456\n","epoch 12: average loss 103.82712619991652\n","epoch 13: average loss 103.361290969976\n","epoch 14: average loss 102.97277955120514\n","epoch 15: average loss 102.7324923863992\n","epoch 16: average loss 102.47972457350792\n","epoch 17: average loss 102.26439151515547\n","epoch 18: average loss 101.95725391929257\n","epoch 19: average loss 101.83075914284224\n","epoch 20: average loss 101.58246855110079\n","epoch 21: average loss 101.42813455368322\n","epoch 22: average loss 101.25648360874895\n","epoch 23: average loss 101.11317770828985\n","epoch 24: average loss 101.02827437004382\n","epoch 25: average loss 100.88233145737688\n","epoch 26: average loss 100.69240902806762\n","epoch 27: average loss 100.65982759351523\n","epoch 28: average loss 100.54198693786519\n","epoch 29: average loss 100.44039501056449\n","epoch 30: average loss 100.3365711896129\n"]}]},{"cell_type":"markdown","source":["With the trained model, we can now run images in the original dataset through the model and see how well they get constructed."],"metadata":{"id":"BQOUaTeoU6bs"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","model.eval()\n","\n","def show_image(x, idx, figure=True):\n","    if figure:\n","      fig = plt.figure()\n","\n","    x = x.view(batch_size, 28, 28)\n","    plt.imshow(x[idx].cpu().numpy())\n","\n","def show_comparison(x, x_hat, idx):\n","    fig = plt.figure()\n","    plt.subplot(1, 2, 1)\n","    show_image(x, idx, False)\n","    plt.title(\"Original\")\n","    plt.subplot(1, 2, 2)\n","    show_image(x_hat, idx, False)\n","    plt.title(\"Reconstruction\")\n","\n","x, _ = next(iter(test_loader))\n","with torch.no_grad():\n","      x = x.view(batch_size, data_dim)\n","      x = x.to(device)\n","\n","      x_hat, _, _ = model(x)\n","\n","show_comparison(x, x_hat, 1) # digit 2\n","show_comparison(x, x_hat, 4) # digit 4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":599},"id":"I0hX4CQ_GBYv","executionInfo":{"status":"ok","timestamp":1716415225103,"user_tz":420,"elapsed":1283,"user":{"displayName":"Adam Majmudar","userId":"11411695890251280564"}},"outputId":"4abfc832-f2f7-4d6a-c68c-690097926429"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["Because of how the decoder learns to create images from the latents (and is forced to represent a lot of the complexity), we can actually create generations from random noise with the decoder and yield distinguishable results.\n","\n","Here, we do this, and I've cherry picked some nice examples (generated from random noise as the latent representations)."],"metadata":{"id":"ZaxEjAJaUl-8"}},{"cell_type":"code","source":["from torchvision.utils import save_image, make_grid\n","\n","with torch.no_grad():\n","    noise = torch.randn(batch_size, latent_dim).to(device)\n","    generated_images = decoder(noise)\n","\n","save_image(generated_images.view(batch_size, 1, 28, 28), 'generated_sample.png')\n"],"metadata":{"id":"o_9TZkZcJFM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_image(generated_images, idx = 6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"mlU08wY4MCm2","executionInfo":{"status":"ok","timestamp":1716415255823,"user_tz":420,"elapsed":4,"user":{"displayName":"Adam Majmudar","userId":"11411695890251280564"}},"outputId":"c427bb25-7533-4d7c-f8fd-b9b3e14e88b1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["show_image(generated_images, idx = 7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"GTf1UceSU2hs","executionInfo":{"status":"ok","timestamp":1716415335899,"user_tz":420,"elapsed":414,"user":{"displayName":"Adam Majmudar","userId":"11411695890251280564"}},"outputId":"037d3e3c-79c6-45f0-85f2-26b3c9ca8e6a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}